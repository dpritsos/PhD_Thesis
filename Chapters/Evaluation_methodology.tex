%!TeX spellcheck = en-US

%\chapter{Evaluation Methodology for WGI and Computational Text Categorization}
\chapter{An Evaluation Framework for Open-set WGI}

\label{chap:eval_methods}

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{Introduction}\label{chap:eval_methods:sec:intro}

This chapter describes a framework suitable for the open-set WGI task. Particularly, the properties of evaluation measures usually adopted in closed-set classification tasks are demonstrated. The sometimes misleading conclusions that can be drawn in case they are also used in open-set conditions are highlighted. To avoid this problem, specific evaluation measures are adopted in this thesis, specialized for the open-set WGI task.

The main difference in open-set WGI with respect to closed-set WGI is the presence of \emph{noise}. As already explained, noise can be \textit{unstructured} (when the labels of web-pages not belonging to any of the known genres are not given) or \textit{structured} (when the labels of web-pages not belonging to any of the known genres are given). Traditional evaluation measures do not make any distinction between known genres and the unknown class (noise). Moreover, in case of structured noise, we need a way to indicate the difficulty of the task taking into account the amount of known and unknown genres. For example, the case where we have 10 known genres and 3 unknown genres is way different than the case where 3 known genres and 10 unknown genres are available. In this thesis we adopt an \textit{openness} measure that specifically quantifies this relation and can be used to thoroughly study the performance of WGI methods in varying conditions.

In the remaining of this chapter, we first describe the properties of well-known evaluation measures usually adopted in supervised learning tasks and discuss their suitability for open-set classification tasks. Then, we focus on appropriate evaluation measures that can depict the performance of open-set classifiers in varying conditions. Finally, the proposed evaluation framework is summarized.

\section{Evaluation Measures}
\label{chap:eval_methods:sec:measures} 

\subsection{Precision, Recall, and $F$-Score}
\label{chap:eval_methods:sec:prf_micro}

In machine learning, specifically in supervised learning, a \textit{confusion matrix} is a table that depicts the performance of an algorithm. It is a special case of a \textit{contingency table}, with two dimensions (i.e., actual and predicted). In the binary classification case, such as depicted in table \ref{chap:eval_methods:tbl:bin_confusion}, there are two classes (i.e., $A$ and $\neg A$ ) and four types of results: True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN). TP and TN correspond to correct predictions while FP and FN are the two types of errors (they are also called Type I and Type II errors).

\begin{table}[t]
	\center
	\caption{The confusion matrix of a binary classification task}\label{chap:eval_methods:tbl:bin_confusion}
	\begin{tabular}{c c c c c}
		& & \multicolumn{2}{c}{Actual} & \\
		\cline{3-4}
		\multirow{3}{*}{\rotatebox[origin=r]{90}{Predicted}} & & \multicolumn{1}{|c}{A} & \multicolumn{1}{c|}{\neg A} &  \\
		\cline{2-4}
		& \multicolumn{1}{|c}{A} & \multicolumn{1}{|c}{\textbf{TP}} & \multicolumn{1}{c|}{FP} & \\
		& \multicolumn{1}{|c}{\neg A} & \multicolumn{1}{|c}{FN} &  \multicolumn{1}{c|}{\textbf{TN}} \\
		\cline{2-4}
		\\
	\end{tabular}
\end{table}

In order to compare the performance of binary classification algorithms, the \tetxit{Accuracy} measure can be used. This is actually the ratio of correct predictions over all available predictions (which is equivalent to the number of the samples of the whole evaluation dataset). Formally, it is defined as follows:

\begin{equation}\label{chap:eval_methods:eq:accuracy}
A = \frac {TP + TN} {TP +  TN + FP + FN}
\end{equation}

Accuracy is heavily influenced by uneven class distribution. Moreover, it gives equal weight to the two types of errors and it cannot handle cases where one of them is more important than the other. In such cases, this evaluation measure can provide misleading conclusions. 

Alternative evaluation measures that can compensate these weaknesses are \textit{Precision} and \textit{Recall}. Precision, also known as \textit{Positive Predictive Value} indicates the fraction of correct predictions for class A over all predictions while recall, also known as \textit{Sensitivity, Hit Rate and True Positive Rate} indicates the fraction of correct predictions for class A over all available instances of this class. These evaluation measures are defined as follows: 

\begin{equation}\label{chap:eval_methods:eq:precision}
	P = \frac {TP} {TP + FP}
\end{equation}

\begin{equation}\label{chap:eval_methods:eq:recall}
	R = \frac {TP} {TP + FN}
\end{equation}

There is a well-known trade-off between precision and recall \parencite{Weiss2010}. Usually when one attempts to optimize one of them the other drops significantly. A popular metric that combines these two measures is called \textit{F-Score} and it is actually the harmonic mean of precision and recall which is increased when both precision and recall take high values and is reduced when at least one of them takes low values. This is defined in the following equation:

\begin{equation}\label{chap:eval_methods:eq:recall}
	F_{\beta} = (1 + \beta^{2}) \frac {P R} {\beta^{2} P + R}
\end{equation}

\noindent
where $\beta$ can be used to regulate the weighting bias towards precision or recall. Usually $\beta = 1$ (i.e., $F_{1}$) is used for equally weighted precision and recall significance. If $\beta > 1$ then recall is more significant than precision and if $\beta < 1$ then precision is more important. This can be useful in specific applications where more emphasis is put on one of these two measures. Note that precision is influenced by FPs while recall is affected by FNs. For example, in email spam detection, precision is usually regarded more important than recall. It is far more important to avoid to miss-classify as spam all legal messages (FPs) than leaving some spam messages to appear in the inbox (FNs).

It is also important to note that precision and recall as well as F-score are calculated for a particular class. So far, taking into account Table \ref{chap:eval_methods:tbl:bin_confusion}, we considered A as the reference class. In general, especially when we have to deal with multi-class classification tasks, precision and recall can be calculated for each class separately. Then, we can combine these measures by taking their arithmetic mean. This provides the \textit{macro-averaged} precision and recall. Let $\mathcal{C}$ be the set of classes in a multi-class classification task (e.g., in WGI this corresponds to the known genre palette) while $P_c$ and $R_c$ are the precision and recall scores of class $c \in \mathcal{C}$, respectively. Then macro-averaged precision recall are defined as follows:

\begin{equation}\label{chap:eval_methods:eq:macro-precision}
	P_{macro} = \frac{1}{|\mathcal{C}|} \sum_{c \in \mathcal{C}}{P_c}
\end{equation}

\begin{equation}\label{chap:eval_methods:eq:macro-recall}
	R_{macro} = \frac{1}{|\mathcal{C}|} \sum_{c \in \mathcal{C}}{R_c}
\end{equation}

\noindent 
where $|\mathcal{C}|$ is the number of known classes. Accordingly, the macro-averaged F-score can be calculated: 

\begin{equation}\label{chap:eval_methods:eq:macro-F}
	F_{macro} = \frac{1}{|\mathcal{C}|} \sum_{c \in \mathcal{C}}{F_c}
\end{equation}

\noindent 
where $F_c$ is the F-score for the class $c \in \mathcal{C}$. Alternatively, one can also calculate \textit{micro-averaged} precision, recall, and F-score. In that case, all data samples are taken together and a single precision and recall value is calculated for all classes cumulatively. TPs correspond to correct predictions, i.e., the diagonal values in the confusion matrix.  All other cells of the confusion matrix are considered as both FPs and FNs (i.e., when a sample of class X is miss-classified to class Y this is a FN for X and a FP for Y). Thus, micro-Precision will be equal to micro-Recall and their harmonic mean  ($F_{1}$) will also be the same. Actually, micro-averaged $F_{1}$ is also equal to the accuracy measure. Consequently, $F_{micro}$ is strongly dependent on the distribution of samples over the classes. On the other hand, $F_{macro}$ gives equal weight to all classes.

\subsection{Open-set Variants of Evaluation Measures}\label{chap:eval_methods:sec:openset_measures}

In an open-set classification task, we are given a set of known classes $\mathcal{C}$ and training samples for each $c \in \mathcal{C}$. However, in the evaluation phase the dataset may consist of both samples belonging to members of $\mathcal{C}$ and samples of classes excluded from $\mathcal{C}$, that is noise $\mathcal{N}$. The latter can be composed of several classes. Especially in WGI, it is expected that the number of web-pages not belonging to any of the known genres would be very high \parentcite{Asheghi2015}. 

If one adopts the evaluation measures described in the previous section for an open-set classification task, then all samples belonging to any $c \notin \mathcal{C}$ could be considered as a single \textit{unknown} class (i.e., a super-class). Then, precision, recall, and $F_{1}$ values can be obtained for this unknown class that would be considered equally important to the corresponding ones of known classes (members of $\mathcal{C}$) when calculating either macro-averaged or micro-averaged precision, recall, and $F_{1}$. However, this implies that TPs for the unknown class are equally important with TPs for a known class. Since there are no training samples for the unknown class and it is actually a merging of several classes, it does not make sense for the evaluation measures to consider this super-class in a  regular way \parencite{mendesjunior2016}. Rather, the open-set evaluation measures should focus on the correct recognition of known classes only. It should be noted that open-set classifiers attempt to recognize the known classes and actually leave unclassified any new samples that are not assigned to any of those classes rather than actually recognizing the unknown classes. This should be reflected in the evaluation measures used to estimate their performance. 

An open-set variant of macro-averaged precision, recall, and $F_{1}$ can be obtained by ignoring the unknown class and calculating the arithmetic mean of only the known classes \parencite{mendesjunior2016}. Formulas \ref{chap:eval_methods:eq:macro-precision}, \ref{chap:eval_methods:eq:macro-recall}, and \ref{chap:eval_methods:eq:macro-F} can still be used. However, it should be underlined that in the open-set scenario the confusion matrix has $|\mathcal{C}|+1$ rows and columns. This means that one class (the unknown class) is ignored when calculating the macro-averaged scores. On the other hand, the samples of the unknown class that are miss-classified to the known classes (false knowns) and the samples of the known classes miss-classified as unknown (false unknowns) still affect the precision and recall of known classes, respectively. It is important to include these errors in the evaluation measures since they actually determine the effect of noise in open-set recognition.

Similar to open-set macro-averaged scores, open-set micro-averaged precision, recall, and $F_{1}$ can be obtained. Note that in this case, micro-precision is not necessarily equal to micro-recall since the former is affected by the presence of false knowns and the latter is affected by false unknowns. Again, the TPs of the unknown class are ignored. 

We provide an illustrating example that demonstrates the difference between traditional evaluation measures and their open-set variants. Table \ref{chap:eval_methods:tbl:multi_confusion} shows an example of a confusion matrix for an open-set classification task with four known classes ($\mathcal{C}=\{A,B,C,D\}$). In WGI, this could correspond to a genre palette of four known genres (e.g. blogs, e-shop, home pages, discussion) for which training samples are available. As can be seen, there are 20 evaluation samples for each known class and 200 samples of the unknown class, or noise ($\emptyset$). This is realistic since in practice noise is expected to outnumber any given known genre. Correct predictions (TPs) are in boldface. The 180 samples of noise correctly left unclassified are the TPs for the unknown class ($\emptyset$). False knowns (see column of $\emptyset$) consist of 20 samples of noise miss-classified to the known classes (i.e., 10 to B and 10 to D) while false unknowns (see row of $\emptyset$) comprise 24 samples of the known classes that are wrongly left unclassified (i.e., 6 from A, 8 from B, and 10 from C). These errors affect the precision and recall of known classes. They correspond to the effect of noise in open-set classification.

Table \ref{chap:eval_methods:tbl:macro_vs_micro} shows the precision, recall, and $F_{1}$ scores for all, both known and unknown, classes. In addition, it demonstrates the traditional macro-averaged and micro-averaged precision, recall, and $F_{1}$ when all classes ($\mathcal{C} \cup \emptyset$) are taken into account as well as their corresponding open-set variant based exclusively on $\mathcal{C}$ ($\emptyset$ is excluded). As can be seen, the unknown class has a particularly high $F_1$ score since most samples not belonging to the known classes where left unclassified. The regular macro-averaged $F_1$ score (i.e., when all classes are included) is positively affected by this. On the other hand, the open-set $F_1$ variant (i.e., when only the known classes are considered) is more realistic since it focuses on the recognition of classes for which there are training examples. By using the regular macro-averaged $F_1$ score in open-set classification, an over-estimation of performance can be obtained. 

This is far more obvious in the case of using micro-averaged $F_1$ scores. In that case, the difference between regular micro $F_1$ and its open-set variant is huge due to the class imbalance problem. As already said, the noise usually outnumbers any known class in WGI tasks and this considerably affects the credibility of micro-averaged measures. It is also noticeable that while regular micro-averaged scores are by definition equal for precision, recall, and $F_1$ (also for accuracy), this is not the case for their open-set variant. 

\begin{table}[t]
	\center
	\caption{An example of confusion matrix of open-set classification}\label{chap:eval_methods:tbl:multi_confusion}
	\begin{tabular}{c c c c c c c c}
		& & \multicolumn{6}{c}{Actual} & \\
		\cline{3-7}
		\multirow{7}{*}{\rotatebox[origin=c]{90}{Predicted}} & & \multicolumn{1}{|c}{A} & \multicolumn{1}{c}{B} & \multicolumn{1}{c}{C} & \multicolumn{1}{c}{D} & \multicolumn{1}{c|}{\emptyset} & Sum \\
		\cline{2-7}
		& \multicolumn{1}{|c}{A} & \multicolumn{1}{|c}{\textbf{13}} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c|}{0} & 15 \\
		& \multicolumn{1}{|c}{B} & \multicolumn{1}{|c}{1} & \multicolumn{1}{c}{\textbf{10}} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c|}{10} & 21 \\
		& \multicolumn{1}{|c}{C} & \multicolumn{1}{|c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{\textbf{8}} & \multicolumn{1}{c}{0} & \multicolumn{1}{c|}{0} & 8 \\
		& \multicolumn{1}{|c}{D} & \multicolumn{1}{|c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{\textbf{20}} & \multicolumn{1}{c|}{10} & 32 \\
		& \multicolumn{1}{|c}{\emptyset} & \multicolumn{1}{|c}{6} & \multicolumn{1}{c}{8} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{0} & \multicolumn{1}{c|}{\textbf{180}} & {204} \\
		\cline{2-7}
		% & & 0 & 14 & 12 & 11 & 12 & 20 & 13 & 16 & \textbf{70}\\
		% \cline{3-11}
		& Sum & 20 & 20 & 20 & 20 & 200 & \\
	\end{tabular}
\end{table}


\begin{table}[t]
	\center
	\caption{Evaluation measures for the example of Table \ref{chap:eval_methods:tbl:multi_confusion}}\label{chap:eval_methods:tbl:macro_vs_micro}
	\begin{tabular}{c c c c}
		\cline{2-4}
		& \multicolumn{1}{|c}{Precision} & \multicolumn{1}{c}{Recall} & \multicolumn{1}{c|}{$F_{1}$}\\
		\cline{1-4}
		\multicolumn{1}{|l}{Class A} & \multicolumn{1}{|c}{0.866} & \multicolumn{1}{c}{0.650} & \multicolumn{1}{c|}{0.743}\\
		\multicolumn{1}{|l}{Class B} & \multicolumn{1}{|c}{0.476} & \multicolumn{1}{c}{0.500} & \multicolumn{1}{c|}{0.488}\\
		\multicolumn{1}{|l}{Class C} & \multicolumn{1}{|c}{1.000} & \multicolumn{1}{c}{0.400} & \multicolumn{1}{c|}{0.571}\\
		\multicolumn{1}{|l}{Class D} & \multicolumn{1}{|c}{0.625} & \multicolumn{1}{c}{1.000} & \multicolumn{1}{c|}{0.769}\\
		\multicolumn{1}{|l}{\textbf{Noise $\emptyset$}} & \multicolumn{1}{|c}{0.882} & \multicolumn{1}{c}{0.900} & \multicolumn{1}{c|}{0.891}\\
		\cline{1-4}
		\multicolumn{1}{|l}{Macro ($\mathcal{C}$)} & \multicolumn{1}{|c}{0.741} & \multicolumn{1}{c}{0.638} & \multicolumn{1}{c|}{0.686}\\
		\multicolumn{1}{|l}{Macro ($\mathcal{C} \cup \emptyset$)} & \multicolumn{1}{|c}{0.770} & \multicolumn{1}{c}{0.690} & \multicolumn{1}{c|}{0.727}\\
		\multicolumn{1}{|l}{Micro ($\mathcal{C}$)} & \multicolumn{1}{|c}{0.632} & \multicolumn{1}{c}{0.600} & \multicolumn{1}{c|}{0.616}\\
		\multicolumn{1}{|l}{Micro ($\mathcal{C} \cup \emptyset$)} & \multicolumn{1}{|c}{0.825} & \multicolumn{1}{c}{0.825} & \multicolumn{1}{c|}{0.825}\\
		\cline{1-4}
	\end{tabular}
\end{table}

Clearly, there is a significant difference between $P_{macro}$ and $P_{micro}$, as well as between $R_{macro}$ and $R_{micro}$. However, note that the change is in opposite direction when regular measures or their open-set variants are used. In particular, open-set $P_{macro}$ is higher than open-set $P_{micro}$ while regular $P_{macro}$ is lower than regular $P_{micro}$. The same pattern applies in recall and $F_1$ scores. This clearly indicates that by adopting regular evaluation measures that are suitable for closed-set classification, it is possible to extract unreliable conclusions.

Note also that, in this example case, the difference between open-set macro-averaged scores and micro-averaged scores does not seem so important. Both approaches seem robust and roughly indicate similar conclusions. However, recall that the samples are evenly distributed over the known classes. If this is not the case, then macro-averaged scores are more reliable since they give equal weight to all known classes. In this thesis, open-set macro-averaged evaluation scores are used.

\subsection{Precision-Recall Curves}\label{chap:eval_methods:sec:roc_prc}

So far, the evaluation measures consider classification algorithms that provide crisp predictions (i.e., \textit{hard} classifiers). The discussed evaluation measures can only show particular aspects of the performance of classifiers. To obtain a deeper look we need richer evaluation methods that can depict the performance of classifiers in a variety of conditions. One such method is the \textit{Precision-Recall Curve} (PRC), a standard method for evaluating information retrieval systems and ranking systems. This approach can only be applied to \textit{soft} classifiers that are able to explicitly estimate class conditional probabilities. Fortunately, the vast majority of hard classifiers can be adopted to also provide some form of score that can be regarded as class conditional probability.

The calculation of a PRC requires the ranking of estimated probabilities in descending order. In each step, the next prediction is considered and a new precision and recall point is calculated. Both macro-averaged and micro-averaged PRC can be calculated. Table \ref{chap:eval_methods:tbl:prc} shows an example of this procedure. As can be seen several samples may have the same certainty score that is used to order predictions. Wrong predictions have as consequence the decrease of precision and recall values.

\begin{table}[t]
	\center
	\caption{Results of a soft classifier ordered by certainty scores. }\label{chap:eval_methods:tbl:prc}
	\begin{tabular}{|c|c|c|c|}
		\cline{1-4}
		Certainty & Predicted & Expected & Correct\\
		\cline{1-4}
		0.99 & A & A & \cmark \\
		0.99 & B & B & \cmark \\
		0.99 & D & D & \cmark \\
		... & ... & ... & ... \\
		0.79 & B & A & \xmark \\
		0.79 & D & D & \cmark \\
		0.69 & A & A & \xmark \\
		0.69 & B & B & \cmark \\
		... & ... & ... & ... \\
		0.64 & \emptyset & B & \xmark \\
		0.60 & B & B & \cmark \\
		... & ... & ... & ... \\
		0.60 & \emptyset & D & \xmark \\
		\cline{1-4}
	\end{tabular}
\end{table}

In order to facilitate the comparison of PRCs corresponding to the performance of different algorithms on the same evaluation dataset, the 11-standard recall level normalization is typically used. The initial points of PRC are reduced to 11 that correspond to standard recall levels $[0,0.1,...,1.0]$. For example, in case Recall$=0.1$, we measure precision when 10\% of the samples belonging to the known classes have been correctly recognized. Precision values are interpolated based on the following formula:

\begin{equation}\label{chap:eval_methods:eq:11recall_level}
	P(r_j)=max_{r_j \leqslant r \leqslant r_{j+1}}(P(r))
\end{equation}

\noindent
where $P(r_j)$ is the precision at $r_j$ standard recall level ($r_j=\{0,0.1,0.2,...,1.0\}$).

The Area Under the Curve (AUC) is a scalar measure that can be extracted from a PRC and can be used to facilitate comparison of different approaches. Certainly, it lacks the details of a PRC but it is useful especially when we want to compare the performance obtained when different parameter settings are applied on the same algorithm. In those cases, both AUC and $F_1$ can be used as optimization criteria. An example of calculating these measures for two systems is provided in Table \ref{chap:eval_methods:tbl:AUC_F1}. In this thesis, we will adopt both of these measures.

\begin{table}[t]
	\center
	\caption{An example of macro-averaged and micro-averaged AUC and $F_1$ of two algorithms}\label{chap:eval_methods:tbl:AUC_F1}
	\begin{tabular}{c c c c c}
		\cline{2-5}
		& \multicolumn{2}{|c|}{Algorithm A} & \multicolumn{2}{c|}{Algorithm B} \\
		\cline{2-5}
		& \multicolumn{1}{|c}{AUC} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c|}{F1} \\
		\hline
		\multicolumn{1}{|c}{Macro($\mathcal{C}$)} & \multicolumn{1}{|c}{0.625} & \multicolumn{1}{c|}{0.754} & 0.499 & \multicolumn{1}{c|}{0.620}\\
		\multicolumn{1}{|c}{Micro($\mathcal{C}$)} & \multicolumn{1}{|c}{0.986} & \multicolumn{1}{c|}{0.750} & 0.999 & \multicolumn{1}{c|}{0.625}\\
		\hline
		\multicolumn{1}{|c}{Macro($\mathcal{C} \cup \emptyset$)} & \multicolumn{1}{|c}{0.612} & \multicolumn{1}{c|}{0.728} & 0.507 & \multicolumn{1}{c|}{0.524}\\	
		\multicolumn{1}{|c}{Micro($\mathcal{C} \cup \emptyset$)} & \multicolumn{1}{|c}{0.986} & \multicolumn{1}{c|}{0.825} & 0.930 & \multicolumn{1}{c|}{0.571}\\
		\hline
	\end{tabular}
\end{table}

In Figure \ref{chap:eval_methods:fig:prc_noise} the PRCs of two different systems are depicted when regular evaluation measures are used (i.e., the unknown class is also considered). Similar, Figure \ref{chap:eval_methods:fig:prc} shows the corresponding performance of the same systems the open-set variants of the evaluation measures are used (i.e., the unknown class is excluded). As can be seen, according to macro-averaged scores, regular measures and open-set variants lead to opposite conclusions. The former favours the grey system while the latter indicates that the red system is better. Clearly, the red system makes less mistakes in the recognition of known genres. In addition, the estimation of performance of both systems with micro-averaged PRCs seems very optimistic.

\begin{center}
    	\includegraphics[scale=0.38]{Figures/prc_macro_micro.eps}
		\caption{Regular precision-recall curve for two systems. Left: Macro-averaged, Right: micro-averaged. \ref{chap:eval_methods:tbl:multi_confusion}}
		\label{chap:eval_methods:fig:prc}
	\end{center}
\end{figure}

\begin{figure}[p]
	\begin{center}
    	\includegraphics[scale=0.38]{Figures/prc_macro_micro_count_noise.eps}
		\caption{Open-set precision-recall curve for two systems. Left: macro-averaged, Right: micro-averaged.}
		\label{chap:eval_methods:fig:prc_noise}
	\end{center}

%Figures \ref{chap:eval_methods:fig:prc_noise} are the PR cures where the noise/unknown prediction are also included (i.e $\mathcal{C} \cup \emptyset$) in the calculation for the scores while in figure \ref{chap:eval_methods:fig:prc} is not. In both figures at the left are the $PR_{macro}$ cures and at the right are the $PR_{micro}$ cures. 

%The $PR_{micro}$ curves are significantly influenced in when the performance of the algorithms in the noise is also included, however in either case the performance high performance shown in the right figures is misleading. Particularly in table \ref{chap:eval_methods:tbl:AUC_F1}, \textit{algorithm B} has a very low $F1_{micro}$ score. Algorithm A is also shown to be overestimated in terms of performance, however, the calculation of the unknown prediction is not changing the estimation of the performance. 

%The main weaknesses of the $PR_{micro}$ curves is its dependency on the score of each corpus sample's which leads to false evaluation in highly imbalanced corpora such the ones of the confusion matrix \ref{chap:eval_methods:tbl:multi_confusion}. Particularly, the problem occurs because the the \textit{total precision} in every recall level is remaining high as long as there are not a lot false positives. The side effect of $PR_{micro}$ curves when they are used of optimization is leading to an algorithm which it is preferring to let most of the corpus samples as unknown.

%The $PR_{macro}$, on the other hand, are better for evaluation because they are class distributions independent. Moreover, they are able to capture the open-set behavior where some of the sample are consider as noise and left as unknown. The difference between the figures \ref{chap:eval_methods:fig:prc_noise} and \ref{chap:eval_methods:fig:prc} is the case of the red line (algorithms A) where it seems to be estimated a bit worst when its performance on noise (i.e.$\emptyset$) it is also calculated.

%In table \ref{chap:eval_methods:tbl:prc} the Area Under the Curve (AUC) of both $PR_{macro}$ curves of algorithms A and B are very slightly defer, however, the $F1_{macro}$ scores are significantly different especially for the algorithms B.


%\section{Area Under the Curve (AUC)}\label{chap:eval_methods:sec:closed_set_classification} 

%In table \ref{chap:eval_methods:tbl:AUC_F1} the Micro and Macro \textit{Area Under the Curve (AUC)} and F1 are presented. The AUC is a common values is used when several $PR$ curves needs to be compared. As an example, to find parameter settings that obtain optimal evaluation performances for the open-set algorithms A and B, the AUC of the $PR$ cures are calculated, shown in figures \ref{chap:eval_methods:fig:prc_noise} and \ref{chap:eval_methods:fig:prc}.

\section{The Openness Test}\label{chap:eval_methods:sec:openness}

The open-set evaluation measures defined in this chapter can be used in both unstructured and structured noise. However, in the structured noise case, we need a more detailed analysis of the performance to evaluate the ability of the open-set classifier to handle low/high number of training/unknown classes. It is especially important to study the relation of the number of training classes with respect to the number of unknown classes. 

In \parencite{scheirer2013toward}, the \textit{openness measure} is introduced for measuring this relation. The openness measure indicates the difficulty of an open-set classification task by taking into account the number of \textit{training classes} (i.e. the known classes used in the training phase) and the number of \textit{testing classes} (i.e., both known and unknown classes used in the testing phase) \parentcite{mendesjunior2016}:

\begin{equation}\label{chap:eval_methods:eq:openness}
	openness=1-\sqrt{\frac{ | Training Classes | }{ |Testing Classes | }}
\end{equation}

When openness is $0.0$, it is essentially a closed-set task, that is the training and testing classes are exactly the same. This actually means that there is no noise. At the other extreme, when openness reaches $1.0$ this means that the known classes are far less than the unknown classes or that the amount of noise is especially high and heterogeneous. Therefore, by varying the openness level we can study the performance of WGI models in different conditions.

Note that the openness measure can only be applied to datasets where all available samples have been labeled with class information. In the case of WGI, we have to know the genre labels of the pages that form the noise (i.e. structured noise). This information is only used to quantify the homogeneity of the noise.

The study of open-set classifiers can be significantly extended by measuring their performance (e.g., macro-averaged $F_{1}$) for varying values of the openness score. Given a dataset with $\mathcal{C}$ set of known classes, it is possible to segment it into two parts: one $K \subset \mathcal{C}$ to serve as the set of known classes and another $S \subset  \mathcal{S}$ ($K \cup U = \mathcal{C}$) to serve as structured noise. Thus, it is possible to vary the training classes from 1 to $|\mathcal{C}|-1$ while the testing classes are always $|\mathcal{C}|$. 

\begin{figure}[t]
	\begin{center}
    	\includegraphics[scale=0.50]{Figures/openness_test_for_f1_scores_example.eps}
		\caption{An example of using the openness test. The smallest openness level corresponds to 4 training classes and 5 testing classes. The maximum openness level refers to 1 training class and 4 testing classes.}
		\label{chap:eval_methods:fig:openness}
	\end{center}
\end{figure}


Figure \ref{chap:eval_methods:fig:openness} depicts the $F1_{macro}$ scores on different openness levels that correspond to two open-set algorithms. A dataset with 5 classes was used to obtain these results. The lowest openness score in that figure corresponds to the case of 4 training classes and 5 testing classes (i.e., noise is composed of a single class). In the highest openness score, there is only one known class and 5 testing classes (i.e., noise is composed of 4 classes). As can be seen, it is possible to conclude that algorithm B is generally better than A. However, algorithm A is better able to cope with the extreme cases of openness.
 

%\section{Domain Transfer Measure}\label{chap:eval_methods:sec:domain_transfer_measure}

%\textit{Domain Transfer Evaluation (DTE)} is a practical methodology for evaluating the classification performance of an text-mining algorithm. The goal of this evaluation methodology is to measure the generalization of the algorithm's induced model when the training corpus is rather small. Thus, with the domain transfer evaluation the algorithm's performance is tested in an unknown domain, for the same text-mining task. 

%Particularly for the WGI, with this measure we can evaluate an algorithm that has been trained to identify \textit{News} or \textit{Wiki} genres. Then by testing it on \textit{Blog}, we could evaluate the model in such a case when very small corpus is available for training. Also, DTE can be applied for evaluating the model's behavior upon changes of the type of \textit{features} have been selected, e.g. BOW, POS, Term N-grams etc. 

%The performance it can be measured using Accuracy, F1-statistic, Precision-Recall Curve, Receiver Operating Characteristic (ROC) Curve etc, and then compare the two measures pairwise for every domain combination (e.g. $\{News, Sports\}$, etc).

%The measure proposed from \parencite{finn2006learning} where equation \ref{chap:eval_methods:eq:office_doc_ensemble} is its generalized form. Originally, this measure was designed for \textit{Accuracy} in mind. However, it can be used for any score such as F1. In order to fit in the open-set framework.

%\begin{equation} \label{chap:eval_methods:eq:office_doc_ensemble}
%	T^{C,F} = \frac{1}{N(N-1)} \sum_{A=1}^{N} \sum_{B, \forall B \neq A}^{N} \left(  \frac{M^{C,F}_{A,B}}{M^{C,F}_{A,A}} \right)
%\end{equation}

%\noindent	
%where T is the \textit{Transfer Measure Score}, M is the measure of choice (Accuracy, F1, Precision, Recall, etc), F is the \textit{Feature Set}, and C is the \textit{Genre Class}. 

\section{Conclusions}

In this chapter we discussed evaluation measures that can be used for open-set classifiers. We demonstrated that traditional precision, recall, and $F_{1}$ measures are misleading since they take into account the unknown class (noise) as a single regular class. However, since this is an heterogeneous class without training examples, it should not be treated equally with the known classes. For that reason, modifications of these evaluation measures, the open-set precision, recall, and $F_{1}$ are more appropriate since the TPs of the unknown class are ignored. For open-set WGI, where the noise is usually highly heterogeneous and significantly outnumbers the known classes, the use of the open-set variants of the measures is considered very important.

Another main direction is the use of graphical evaluation methods that better depict the performance of open-set classifiers in various conditions. We suggest the use of two such measures, the precision-recall curves on 11-standard recall levels and the openness test. The former provides a detailed view of the performance of an open-set WGI system that suits any given application (e.g., in ranking applications, precision at low recall levels is of paramount importance). The latter provides a direct control of the difficulty of the task in structured noise and can demonstrate the performance of the classifiers in varying conditions, from cases very similar to the closed-set scenario where noise is homogeneous to cases very similar to binary classification where the structured noise is highly heterogeneous.

In the next chapters we will adopt the evaluation principles described here to evaluate the open-set WGI algorithms introduced in this thesis.