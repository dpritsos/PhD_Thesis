%!TeX spellcheck = en-US

%\chapter{Evaluation Methodology for WGI and Computational Text Categorization}
\chapter{Evaluation framework for open-set WGI}

\label{chap:eval_methods}

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{Introduction}\label{chap:eval_methods:sec:intro}

This chapter is describing the evaluation metrics required for the open-set WGI task. Particularly it is shown with simple examples the effect of the a measurement methods to the evaluation results and the misleading conclusions on can have when the wrong measurement method is adopted. Moreover some evaluation measures are presented specialized for the open-set framework where recently have been discover and adopted for several domains inside and outside the \tetxit{text minminng} domain.

The standard evaluation approach is to use a previously well tested evaluation methodology and measures. However, the closed-set measures is later shown that they are not proper for the open-set framework in their standard form. 

To reason it in few words the problem is that these measures are based on the \tetxit{Confusion Matrix} which can capture only two condition per variable. However, in the case of open-set framework there is one say "global" condition where all the variables of the table drops. This condition is occurring when the \tetxit{rejection condition of the open-set algorithm is trigered. That is when an open-set algorithm responds "I don't know" for an arbitrary sample. 

As expled in chapter \ref{chap:openset} there are some effortes to crate open-set algorithms than could model the \tetxit{unkown} or \textit{unstructured noise} samples. In this case it could be possible the out-of-the-box closed-set evaluation methods be sufficent,however, in most ML algorithms where a rejection critirion is considered this is nearly imposible as it will be presented later.

\section{Closed-set vs Open-set Measures}\label{chap:eval_methods:sec:measures} 

In this section it is presented how the text mining evaluation measures are affected by the type of the by the type of the task's approach scenario. That is, whether i
t is an open-set of closed-set classification framework.

Starting with the \textit{Confusion matrix} where the basic closed-set evaluation measures are based it is shown the diffence in measure of the Precision and Macro-Precision diffence (also for Recall) in the open-set framework. Moreover, it is shown how these measures are affected by he openness score which is the score for evaluating the difficulty level of an open-set problem.

Finally, the \tetxit{Open Space Risck} is presented which is the measure helping an open-set algorithm to regulate the potentially error in the presense of \tetxit{noise} in the testing or/and \tetxit{outages} in the training phase.


\subsection{Confusion Matrix and $F$ Score}\label{chap:eval_methods:sec:prf_micro}

In Machine Learning (and Statistical) Classification, a \textit{Confusion Matrix} is a table that depicts the performance of an algorithm. It is a special case of a \textit{Contingency Table}, with two dimensions, actual and predicted classes. Thus, in the a binary case such as in the table \ref{chap:eval_methods:tbl:bin_confusion}, there are for cases occuring. True Positive (TP), True Negative (TN), False Postive (FP), False Negative (FN).

In the case of binary classification the sampels under the class distribution A are desired, i.e. positive, and all the other samples outside the distribution are considered negative, classified in say class B. Then TP and TN are the counts of samples predicted corectly under the a class A (positive) or class B (negative). Therefore, the FP and FN are the samples a binary algorithm missleaded and predicted the wrong class. 

\begin{table}[H]
	\center
	\caption{Confusion matric of binary classification}\label{chap:eval_methods:tbl:bin_confusion}
	\begin{tabular}{c c c c c}
		& & \multicolumn{2}{c}{Actual} & \\
		\cline{3-4}
		\multirow{3}{*}{\rotatebox[origin=r]{90}{Predicted}} & & \multicolumn{1}{|c}{A} & \multicolumn{1}{c|}{B} & \\
		\cline{2-4}
		& \multicolumn{1}{|c}{A} & \multicolumn{1}{|c}{\textbf{50}} & \multicolumn{1}{c|}{25} & 75 \\
		& \multicolumn{1}{|c}{B} & \multicolumn{1}{|c}{20} & \multicolumn{1}{c|}{\textbf{45}} & 65 \\
		\cline{2-4}
		&  & 70 & 70 & \textbf{95}\\
	\end{tabular}
\end{table}

In order to measure the performance a binary classification algorithm the \tetxit{Accuracy} is a common measure which is actually the ration of all the correct prediction overall the predictions (which is equivalent to the number of the samples of the whole data set). Formally, it is shown in the equation \ref{chap:eval_methods:eq:accuracy} and for the example of the table \ref{chap:eval_methods:tbl:bin_confusion} its value is $A = 95/140 = 0.68$

\begin{equation}\label{chap:eval_methods:eq:accuracy}
A = \frac {TP + TN} {TP +  TN + FP + FN}
\end{equation}

However, one can get a better insight when it is measured the algorithms ability distinguishing the samples which they are under the distribution it has been trained for. Moreover, to be measured its ability of recognizing the samples of this distribution. The former is called \tetxit{Precision} and the second it is called \tetxit{Recall} and their formal expressions are in equation \ref{chap:eval_methods:eq:precision} and \ref{chap:eval_methods:eq:recall} respectively. 

\begin{equation}\label{chap:eval_methods:eq:precision}
	P = \frac {TP} {TP + FP}
\end{equation}

\begin{equation}\label{chap:eval_methods:eq:recall}
	R = \frac {TP} {TP + FN}
\end{equation}

\tetxit{Precision} is also known as \tetxit{Positive Predictive Value (PPV)}. \textit{Recall} is also known as \textit{Sensitivity, Hit Rate and True Positive Rate (TPR)}. 

Second multiclass with 7 var closed set

third multiclass open set 





\subsection{Macro-Precision and Macro-Recall}\label{chap:eval_methods:sec:prf_macro}



\subsection{PRC and ROC Curves}\label{chap:eval_methods:sec:roc_prc}



\section{Area Under the Curve (AUC)}\label{chap:eval_methods:sec:closed_set_classification} 

Precision-Recall curve is a standard method to visualize the performance of classifiers. In this paper, the Precision-Recall curve is calculated in 11-standard recall levels $[0,0.1,...,1.0]$. Precision values are interpolated based on the following formula:

\begin{equation}
	P(r_j)=max_{r_j \leqslant r \leqslant r_{j+1}}(P(r))
\end{equation}

\noindent
where $P(r_j)$ is the precision at $r_j$ standard recall level.


To compensate the potentially unbalanced distribution of web pages over the genres, we are using the macro-averaged precision and recall measures. In more detail, we use the modified version of precision and recall for open-set classification tasks proposed by \parencite{mendesjunior2016}. This modification calculates precision and recall only for the known classes (available in the training phase) while the unknown samples (belonging to classes not available during training) affect false positives and false negatives. To find parameter settings that obtain optimal evaluation performances we use 2 scalar measures, the Area Under the Precision-Recall Curve (AUC)and $F_{1}$. We will show that the appropriate selection of the optimization measure is highly significant in the presence of noise.


\section{Re-defining the Open Space Risk}\label{chap:eval_methods:sec:open_space_risk} 

The open space risk in \parencite{scheirer2013toward} is originally defined as in eq. \ref{chap:eval_methods:eq:the_original_open_space_risk}

\begin{equation}\label{chap:eval_methods:eq:the_original_open_space_risk}
	R_{o}(f) = \frac{\int_{o} f_{y}(x) dx}{\int{S_{o}}  f_{y}(x) dx}

\end{equation}

\noindent
where $R_{o}(.)$ is the open-space risk function and $f_{y}(x)  \in \{0, 1\}$ is the classification function of class $y$, where $1$ is for recognizing its class and $0$ when not. $S_{o}$ is the large hyper-sphere where all the positive training data points and the \textit{positive open space area} $O$. 

The original formulation of the eq. \ref{chap:eval_methods:eq:the_original_open_space_risk} $O$ area cannot be constrained by any means. The only information we are getting is the farther form the training date we go the risk of miss-classification is increasing One method to constrain the problem is by using the center of the positively labeled training data and defining a radios $r_{o}$ where it will reduce the open space area based on the positively labeled empirically observations. Then the $O$ is defined by the equation eq. \ref{chap:eval_methods:eq:openspace_spherical_constrained}

\begin{equation}\label{chap:eval_methods:eq:openspace_spherical_constrained}
	O = S_{o} - B_{r_{y}}(C_{y})
\end{equation}

\noindent
where $B_{r_{y}}(.)$ is the function which defines the area of radius $r_{y}$ of the $C_{y}$ class defined by its training data \parencite{fei2016breaking}.

\section{Openness test}\label{chap:eval_methods:sec:open_space_risk}

In \parencite{scheirer2013toward} work in image processing, the \textit{openness measure} is introduced, as shown in eq. \ref{chap:eval_methods:eq:openness}. The openness measure indicates properties of an open-set classification task by taking into account the number of \textit{training classes}, i.e. the known labels used in the training phase and the number of \textit{testing classes}, i.e., the labels, both known and unknown, used in the testing phase.

\begin{equation}\label{chap:eval_methods:eq:openness}
	openness=1-\sqrt{\frac{ | Training Classes | }{ |Testing Classes | }}
\end{equation}

When openness is $0.0$, it is essentially a closed-set task, that is the training and testing classes are the same or there is no noise. When openness reaches $1.0$ this means that the known classes are far less than the unknown classes, that is the amount of noise is especially high. Therefore, by varying the openness level we can study the performance of WGI models in different conditions.

Note that the openness measure can only be applied to corpora where all available documents have been labeled with genre information. In other words, we have to know the genre labels of the pages that form the noise (i.e. structured noise). Thus, it cannot be applied to SANTINIS corpus where the web pages taken from the SPIRIT collection are unclassified (i.e., unstructured noise). On the other hand, the SANTINIS corpus provides the opportunity to examine WGI performance when all documents not belonging to the known labels are grouped into one single (highly heterogeneous) class.



\section{Domain Transfer Measure}\label{chap:eval_methods:sec:domain_transfer_measure}

A practical methodology for evaluating a classification/identification ML model in a text-categorization task is the \textit{Domain Transfer Evaluation}. The goal of this evaluation methodology is to measure the generalization of the model when training corpus is rather small and to evaluate how the model would perform in an unknown domain for the same task. 

Particularly for the AGI/WGI with this measure we can evaluate a ML algorithm when for example the model has been trained to identify \textit{News} and \textit{Wiki} genres, however, the available corpus would be only from \textit{Technology products Topics}. Then by testing it on {Sports Topics} we could evaluate the model in such a case when very small corpus is available for training. In addition using this methodology we can evaluate the models behavior depending on the \textit{Features} have been selected for the training, e.g. BOW, POS, Term N-grams etc. 

One can measure the performance, say Accuracy, F1-statistic, Precision-Recall Curve,  Receiver Operating Characteristic (ROC) Curve etc, and then compare the two measures pairwise for every domain combination (e.g. $\{Mobile Phones, Football\}$, etc). However, it would be easier to have measure for all possible combinations training/testing of different domain combinations. 

The measure proposed from \parencite{finn2006learning} and shown in equation \ref{eq:gnr_dom_transit_general} in its generalized form. Originally, this measure was designed for Accuracy measure in mind. However, it can be used for any measure say $F_{1}$-statistic in order to fit in open-set framework and not respected to the closed-set also (Να ελέγξω αν το Accuracy μπορεί να χρησιμοποιηθεί για Open-set). 

\begin{equation} \label{chap:eval_methods:eq:office_doc_ensemble}
	T^{C,F} = \frac{1}{N(N-1)} \sum_{A=1}^{N} \sum_{B, \forall B \neq A}^{N} \left(  \frac{M^{C,F}_{A,B}}{M^{C,F}_{A,A}} \right)
	\end{equation}

	\noindent	
where T is the \textit{Transfer Measure Score}, M is the measure of choice (Accuracy, $F_1$, Precision, Recall, etc), F is the \textit{Feature Set}, and C is the \textit{Genre Class}. 
























