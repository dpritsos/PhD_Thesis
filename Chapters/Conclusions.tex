%!TeX spellcheck = en-US

\chapter{Conclusions}

\label{chap:conclusions}

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{Introduction}

WGI is a text mining task that can improve information retrieval systems allowing richer description of search queries and results. Genre is orthogonal to topic of documents and the combination of information about these two factors better describe the properties of documents in comparison to the case only topic is used. WGI can be useful to build specialized collections of documents via focused crawling where both topic and genre are controlled. Genre is important information for credibility assessment systems that decide whether certain web-pages can be trusted as well as for anti-phishing systems. In addition, genre of documents is important information in order to apply suitable models when NLP technology is going to be used.

Despite these interesting uses of WGI, research in this area is limited. In addition, the vast majority of existing works adopt the unrealistic scenario of closed-set classification. Clearly, it is not possible to define a universal genre palette including all possible web genres. Despite best efforts, it is expected that a large portion of web-pages could not match a given genre palette no matter how long it is. In addition, web genres evolve through time and the genre palette should be re-defined periodically.

The adoption of the closed-set classification scenario provides an over-estimation of the obtained results and does not permit the highlighting of weaknesses and strengths in real world conditions. Moreover, the few open-set WGI models that have been proposed so far have not been adequately tested. In some cases the evaluation is based on unrealistic noise-free corpora. In other cases, regular evaluation measures that are not suitable for open-set classification tasks are adopted. 

Another crucial issue is the type of noise considered in WGI tasks. So far, only unstructured noise has been tested. However, structured noise is equally important since it provides the opportunity to examine the behaviour of an open-set model controlling the homogeneity of noise and the difficulty of the problem. 

As a result, there are significant doubts about the applicability of WGI technology in challenging environments, outside controlled lab experiments. It is also important to note that not all WGI applications are the same. Some applications (e.g., ranking of search results by genre, focused crawling) require higher precision than recall. It is crucial, therefore, to demonstrate what methods are better able to deal with such cases.

In the remaining of this Chapter, we present the main findings of this thesis and the suggested direction for future work.

\section{Main Findings}

The focus of this thesis is on open-set WGI. A series of methods have been described and experiments demonstrating their strengths and weaknesses have been reported using a suggested evaluation framework for open-set classification tasks. The main findings of this study are following:

\begin{itemize}
    \item An evaluation framework for open-set WGI is proposed. First, as clarified in Chapter \ref{chap:eval_methods}, the evaluation measures used for closed-set classification tasks are not appropriate for open-set conditions. The focus of the evaluation measures should be on the recognition of known genres. The recognition of the unknown class should not affect the results since there are no training examples for that class. In addition, in case micro-averaged scores are used, the performance estimation is skewed when the unknown class is regularly taken into account since it is the majority class. Open-set variants of precision, recall, and $F_1$ (i.e., excluding the unknown class) should be used. The effect of noise is thus based only on false unknowns and false knowns errors (i.e., true positives of the unknown class are ignored). In addition, graphical models, like precision-recall graph in 11-standard recall levels provide a detailed view on the properties of examined methods that match requirements of a given WGI application. 
    \item The effect of structured noise in WGI is studied. We demonstrate in Chapter \ref{chap:eval_methods} that the openness measure provides a means to control the homogeneity of structured noise. This also affects the difficulty of the open-set task. By using the openness test it is possible to see how open-set WGI methods behave in extreme conditions (e.g. when very few known classes are available or when noise is quite homogeneous) and better understand their pros and cons.
    \item An open-set WGI method following the one-class classification paradigm has been developed. The OCSVM approach learns only from positive samples of a target class and a hyper-parameter ($\nu$) adjusts whether the model will be conservative or optimistic. It is a general-purpose approach that can be applied to any open-set classification task as it has already been demonstrated in \parentcite{mendesjunior2016}. In the framework of this thesis, it serves as a baseline to other more suitable approaches. The results described in Chapter \ref{chap:noise} demonstrate that OCSVM is a conservative model that prefers to leave samples unclassified rather than miss-classify them. The performed experiments also showed that this method outperformed the other examined approaches when the openness level is quite high. This means that when very few known classes are available and the noise is heterogeneous (i.e., this corresponds to a high open-space risk) this relatively conservative method obtains the best results.
    \item An open-set WGI method following the ensemble paradigm (RFSE) has been developed. This approach takes advantage of a high-dimensional feature space with lots of redundant and irrelevant features such as the ones provided by character and word n-grams. This method achieves the best overall results in terms of $F_1$ as demonstrated in Chapter \ref{chap:noise}. It provides the most balanced case of handling both unstructured and structured noise. It is also possible to favour recall or precision by using different parameter optimization criteria (i.e., AUC or $F_1$). Thus, it can be adapted to the requirements of WGI applications. In addition, for very high openness values this algorithm does not seem to be robust. This actually means that it cannot easily handle cases with very few known genres. Its performance increases by enlarging the known genre set. 
    \item An open-set WGI method following the k-nearest neighbor approach (NNDR) has been developed \parentcite{mendesjunior2016}. In contrast to kNN, this method spends time during training to estimate the open-space risk. In order to do that, part of training examples are used as simulated noise. On the other hand, this method is very difficult (or even unfeasible) to be applied when limited training data (especially when limited known genres) are available. This is the main reason that it has been evaluated only in conditions of unstructured noise in this thesis. Another important property of the algorithm is that it is better able to handle compact and dense representations rather than high-dimensional and sparse feature spaces. Thus, it combines very well with distributed representations acquired from neural language modeling as demonstrated in Chapter \ref{chap:word_embeddings}. Although the best results of this method are inferior to the ones obtained by RFSE, NNDR seems to provide better precision scores for low recall levels. The latter is very important in applications related with ranking of web-pages.
    \item The experiments conducted in this thesis are based on textual features only. These have been found in previous studies to be the most useful source of information to represent genre-related information. Features like word and character n-grams have been examined. They provide a language-independent and easy-to-measure set of features. More elaborate features requiring analysis of documents by NLP tools, like POS n-grams have also been tested. As the results of Chapter \ref{chap:noise} indicate, word n-grams provide the best results. More specifically, word trigrams seem to be better than word unigrams. This also indicates that content and stylistic information is useful for the specific corpora used in the experiments. Recall that these corpora are far from ideal for evaluating WGI methods since they are of small size and some genres are represented by thematically correlated text samples. This could mean that the conclusion that best results are obtained for open-set WGI methods using word trigrams is probably corpus-dependent. The type of features that should be used in combination with the proposed methods, especially RFSE, is a hyper-parameter that should be optimized for any given corpus.
    \item Distributed representations obtained by the PV-DBOW neural network language model has been introduced to WGI. As demonstrated in Chapter \ref{chap:word_embeddings}, these features when combined with NNDR can provide a very competitive approach especially when precision is the most important aspect of performance. Distributed representations are compact and dense and cannot easily confuse NNDR with irrelevant and redundant features. In addition, it has been demonstrated that NNDR can be a very robust model when matched with distributed features requiring minimal tuning of hyper-parameters.
\end{itemize}

\section{Future Work Directions}

The current work attempted to highlight weaknesses of existing approaches and provide a framework for the appropriate evaluation of open-set WGI approaches. In addition, specific open-set methods have been tested using this framework. There are several open questions that are outside the scope of this work and could be examined in the future. Promising directions for research include the following:

\begin{itemize}
    \item The open-set classifiers examined in this thesis can be combined in the framework of a heterogeneous ensemble. This could provide a more robust approach of enhanced performance, both overall and in low recall levels.
    \item Open-set methods proposed in other domains, like the one described in \parentcite{fei2016breaking}, could be tested in WGI as well. Distance-based approaches seem to fit WGI tasks. 
    \item Alternative neural network language modeling approaches could be used to provide distributed representations. These include pre-trained language models, like BERT, ULMFiT, ELMo, and GPT-2 that have been developed recently and obtained excellent results in several text classification tasks \parentcite{Devlin:2019,Howard:2018,Peters:2018,radford2019language}.
    \item Combinations of textual features with alternative sources of information, like structural or graph-based and URL-based features could be examined. It would be interesting to see how such enlarged feature spaces could affect methods like RFSE and NNDR.
    \item Additional experiments can be performed on new web corpora including dozens of genres and thousands of training examples per genre \parentcite{Egbert:2015}. This would allow a more objective evaluation of open-set WGI approaches especially focusing on the effect of structured noise using the openness test. On the other hand, this possibility is limited given the copyright issues that complicate access to recently-developed collections \parentcite{Asheghi2015}.
    \item Case studies examining the usefulness of open-set WGI in increasing effectiveness of specific applications, like genre-aware crawling, credibility assessment systems should be performed \parentcite{deAssis:2017,Agrawal:2018}. This would both study the robustness of open-set WGI in real world conditions and would trigger the interest of research community for further research.
    
\end{itemize}

