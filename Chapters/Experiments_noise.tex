%!TeX spellcheck = en-US

%\chapter{Handling the Noise of the Web-Genres}
\chapter{Experimental Open-set Framework Effectiveness Evaluation on Noise}

\label{chap:noise}

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{Introduction}\label{chap:noise:sec:intro}

\section{Noise vs Outages on Open-set Classification}\label{chap:noise:sec:noise_vs_outages}

\section{Open-set Framework Evaluation on Noise}\label{chap:noise:sec:openset_evaluation}

%\section{Open-set Ensembles for WGI with Noise}\label{chap:noise:sec:openset_ensembles}


\section{Experimental Setup}\label{chap:noise:sec:experimental_setup}

\subsection{Corpora}\label{chap:noise:sec:corpora}
In this paper we study the performance of the open-set classification models on the WGI task. In particular, the two open-set algorithms described above are analytically tested on benchmark corpora. In particular, our experiments are based on the following corpora already used in previous work in WGI \parencite{meyer2004genre,santini2007automatic,kanaris2009learning}:

\begin{enumerate}
	\item \textit{SANTINIS} \parencite{mehler2010genres_on_web}: This is a corpus comprising 1,400 English web pages evenly distributed into 7 genres as well as 80 BBC web pages evenly categorized into 4 additional genres. In addition, it comprises a random selection of 1,000 English web pages taken from the SPIRIT corpus \parencite{joho2004spirit}. The latter can be viewed as noise in this corpus. Details are given in table \ref{chap:noise:tbl:genre_tags}.
	\item \textit{KI-04} \parencite{meyer2004genre}: This is a collection of 1,205 English web pages unevenly categorized into 8 genres. Details can be seen in table \ref{chap:noise:tbl:genre_tags}.
\end{enumerate}


Our text representation features are based exclusively on textual information from web pages excluding any structural information, URLs, etc. Based on the good results reported in \parencite{sharoff2010web,pritsos2013open,Asheghi2015} as well as some preliminary experiments, the following document representation schemes are examined: Character 4-grams (C4G), Word unigrams (W1G), and Word 3-grams (W3G). We use the Term-Frequency (TF) weighting scheme and the feature space is defined by a \textit{Vocabulary} which is extracted based on the terms appearing at training set only.

As concerns OCSVM model, two parameters have to be tuned: the number of features $F$ and $\nu$. For the former, we used $F=$\textit{\{1k, 5k, 10k, 50k, 90k\}}, of most frequent terms of the vocabulary. Following the reports of previous studies \parencite{scholkopf1999estimating} and some preliminary experiments, we examined $\nu$\textit{=\{0.05, 0.07, 0.1, 0.15, 0.17, 0.3, 0.5, 0.7, 0.9\}}. In comparison to \parencite{pritsos2013open}, this set of parameter values is more extended. With respect to RFSE, four parameters should be set: the vocabulary size $F$, the number of features used in each iteration $fs$, the number of iterations \textit{I}, and the threshold $\sigma$. We examined $F$\textit{=\{5k, 10k, 50k, 100k\}}, $fs$=\textit{\{1k, 5k, 10k, 50k, 90k\}}, \textit{I}=\textit{\{10, 50, 100\}} (following the suggestion in \parencite{koppel2011authorship} that more than 100 iterations does not improve significantly the results) and $\sigma$\textit{=\{0.5, 0.7, 0.9\}} (based on some preliminary tests). Additionally, in this work we are testing three document similarity measures: cosine similarity, MinMax similarity,  and combined cosine similarity and MinMax. Finally, to extract the best possible parameter settings for each classification method we apply grid-search over the space of all parameter value combinations.

\begin{table}
\center
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{2}{|c|}{SANTINIS} & \multicolumn{2}{c|}{KI-04}\tabularnewline
\hline
\multicolumn{1}{|c|}{Genre} & \multicolumn{1}{c|}{Pages} & \multicolumn{1}{c|}{Genre} & \multicolumn{1}{c|}{Pages}\tabularnewline
\hline
\multicolumn{1}{|l|}{Blog} & \multicolumn{1}{c|}{200} & \multicolumn{1}{l|}{Article} & \multicolumn{1}{c|}{127}\tabularnewline
\multicolumn{1}{|l|}{Eshop} & \multicolumn{1}{c|}{200} & \multicolumn{1}{l|}{Discussion} & \multicolumn{1}{c|}{127}\tabularnewline
\multicolumn{1}{|l|}{FAQ} & \multicolumn{1}{c|}{200} & \multicolumn{1}{l|}{Download} & \multicolumn{1}{c|}{152}\tabularnewline
\multicolumn{1}{|l|}{Frontpage} & \multicolumn{1}{c|}{200} & \multicolumn{1}{l|}{Help} & \multicolumn{1}{c|}{140}\tabularnewline
\multicolumn{1}{|l|}{Listing} & \multicolumn{1}{c|}{200} & \multicolumn{1}{l|}{Link Collection} & \multicolumn{1}{c|}{208}\tabularnewline
\multicolumn{1}{|l|}{Personal Home Page} & \multicolumn{1}{c|}{200} & \multicolumn{1}{l|}{Portrayal-Non Private} & \multicolumn{1}{c|}{179}\tabularnewline
\multicolumn{1}{|l|}{Search Page} & \multicolumn{1}{c|}{200} & \multicolumn{1}{l|}{Portrayal- Private} & \multicolumn{1}{c|}{131}\tabularnewline
\multicolumn{1}{|l|}{DIY Mini Guide (BBC)} & \multicolumn{1}{c|}{20} & \multicolumn{1}{l|}{Shop} & \multicolumn{1}{c|}{175}\tabularnewline
\multicolumn{1}{|l|}{Editorial (BBC)} & \multicolumn{1}{c|}{20} &  & \tabularnewline
\multicolumn{1}{|l|}{Features (BBC)} & \multicolumn{1}{c|}{20} &  & \tabularnewline
\multicolumn{1}{|l|}{Short Bio (BBC)} & \multicolumn{1}{c|}{20} &  & \tabularnewline
\multicolumn{1}{|l|}{Noise (Spirit1000)} & \multicolumn{1}{c|}{1000} &  & \tabularnewline
\hline
\end{tabular}
\caption {Corpora descriptions and amount of pages per genre.}
\label{chap:noise:tbl:genre_tags}
\end{table}


\section{Experiments}\label{chap:noise:sec:Experiments_Results}

\subsection{WGI with Unstructured Noise}\label{chap:noise:sec:WGI_noise}

The two open-set algorithms RFSE and OCSVME, describe in sections \ref{chap:openset:sec:alg:OCSVM_Ensemble} and \ref{chap:openset:sec:alg:RFS-Ensemble}, are initially tested on SANTINIS corpus which as explained above is an Unstructured Noise, samples corpus.   

In the training phase, only the 11 known genres are considered. In the testing phase, the noise pages coming from the SPIRIT corpus are also used. It is important to be noted that information about the true genre of these pages is not available. The 10-fold cross validation is performed where in each fold the full set of 1,000 pages of noise is included. This evaluation strategy is giving a more realistic evaluation framework since the size of the noise is much greater than the size of any genre included in the given palette.

Figures \ref{chap:noise:fig:MacroPRC_OCSVME_W3G_W1G_C4G_OPTIMAL_SANTINIS} and \ref{chap:noise:fig:MacroPRC_RFSE_W3G_W1G_C4G_OPTIMAL_SANTINIS} depict the Precision-Recall curves (PRC) of OCSVM and RFSE models, respectively. For each model and each one of the three document representations, the parameters that maximize performance with respect to the $F_{1}$-measure are used. Remember from section \ref{chap:eval_methods:sec:roc_prc} whenever recall does not reach 1.0 this means that some pages belonging to known classes were classified as unknown. 

In all cases, RFSE outperforms OCSVM. Moreover, for both methods, W3G seems to be the best feature type for this corpus, followed by C4G. OCSVM performance is only comparable with RFSE when W3G is used.

\hfill \break


\begin{figure}[H]
	\begin{center}
    \includegraphics[scale=0.45]{Figures/OCSME_Best_per_DocRep.eps}
	\caption{Precision-Recall Curves of OCSVM models on SANTINIS corpus using W1G, W3G, and C4G features.}
	\label{chap:noise:fig:MacroPRC_OCSVME_W3G_W1G_C4G_OPTIMAL_SANTINIS}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
    \includegraphics[scale=0.45]{Figures/RFSE_Best_per_DocRep.eps}
	\caption{Precision-Recall Curves of RFSE models on SANTINIS corpus using W1G, W3G, and C4G features.}
	\label{chap:noise:fig:MacroPRC_RFSE_W3G_W1G_C4G_OPTIMAL_SANTINIS}
	\end{center}
\end{figure}

The performance of the open-set WGI methods are further explored by selecting parameter settings with different optimization criteria. Tables \ref{chap:noise:tbl:OCSVME_SANTINIS} and \ref{chap:noise:tbl:RFSE_SANTINIS} show the combination of parameters that optimize performance of OCSVM and RFSE based on AUC, $F_{1}$ and $F_{0.5}$. 

In the tables \ref{chap:noise:tbl:OCSVME_SANTINIS} and \ref{chap:noise:tbl:RFSE_SANTINIS} the values are presented, of all three performance measures where, for every row, one of them is maximized. It is clear that the performance in all cases is maximized when W3G document representation is used. In previous studies based on a closed-set framework, C4G was the document type of features to maximize performance \parencite{Sharroff2010}. This indicates that contextual and content information is important for this corpus \parencite{Asheghi2015}.

In addition, in almost all cases, RFSE models are far more effective than OCSVM. Another important conclusion is that the optimization criterion plays a crucial role for the properties of the model especially for RFSE. When AUC is maximized, recall is favored. On the other hand, while $F_{1}$ is maximized, precision is substantially increased. Fig. \ref{chap:noise:fig:MacroPRC_RFSE_OCSVME_SANTINIS} shows the performance of OCSVM and RFSE models when AUC and $F_{1}$ criteria are used to select parameter settings. As can be seen, the RFSE model based on $F_{1}$ maximization avoids to make wrong decisions and leaves a large number of web pages unclassified. On the other hand, the model optimized by AUC prefers to make a lot of errors in order to recognize more web pages of known genres. OCSVM models seem not significantly affected. Note that choosing between WGI models that prefers precision over recall and vice versa is an application-specific task.

\begin{figure}[H]
\begin{center}
    \includegraphics[scale=0.45]{Figures/MacroPRC11AVG_RFSE_OCSVME_SANTINIS_2.eps}
	\caption{Precision-Recall Curves of OCSVM and RFSE models on SANTINIS corpus optimized either by AUC or $F_{1}$.}
	\label{chap:noise:fig:MacroPRC_RFSE_OCSVME_SANTINIS}
	\end{center}
\end{figure}

% 7Genres
%
\begin{table}[H]
\centering

\pgfplotstableset{
    create on use/Criterion/.style={ create col/set list={AUC, $F_{1}$, $F_{0.5}$} },
	columns/DocRep/.style={string type},
	create on use/DocRep/.style={ create col/set list={W3G, W3G, W3G} },
	columns/DocRep/.style={string type}
}

\pgfplotstabletypeset[
		fixed,
		precision=3,
		col sep=comma,
		every head row/.style={
			before row = \toprule,
			after row=\midrule,
		},
		every last row/.style={after row=\bottomrule \\},
		columns={Criterion, DocRep, 0, 1, 2, 3, 4, 5, 6, 7},
		columns/Criterion/.style ={string type,column type=c, column name=Optim.},
        columns/DocRep/.style ={string type,column type=c, column name=Features},
		columns/0/.style ={column type=c, column name=Voc.},
		columns/1/.style ={column type=c, column name=\textit{f}},
		columns/2/.style ={column type=c, column name=$\nu$},
		columns/3/.style ={column type=c, column name=Prec.},
		columns/4/.style ={column type=c, column name=Rec.},
		columns/5/.style ={column type=c, column name=AUC},
        columns/6/.style ={column type=c, column name=$F_{0.5}$},
        columns/7/.style ={column type=c, column name=$F_{1}$},
		]{tables_data/AUC_FStatistics_tables/OCSVME_SANTINIS_Best.csv}
\caption{Best performing models for OCSVM on SANTINIS corpus.}
\label{chap:noise:tbl:OCSVME_SANTINIS}
\end{table}

\begin{table}[H]
\centering

\pgfplotstableset{
    create on use/Criterion/.style={ create col/set list={AUC,$F_{1}$,$F_{0.5}$} },
    columns/DocRep/.style={string type},
	create on use/DocRep/.style={ create col/set list={W3G,W3G,W3G} },
	columns/DocRep/.style={string type},
    create on use/SimMeas/.style={ create col/set list={Combo,MinMax,MinMax} },
	columns/SimMeas/.style={string type}
}

\pgfplotstabletypeset[
		fixed,
		precision=3,
		col sep=comma,
		every head row/.style={
			before row = \toprule,
			after row=\midrule,
		},
		every last row/.style={after row=\bottomrule \\},
		columns={Criterion, DocRep, SimMeas, 0, 1, 2, 3, 4, 5, 6, 7, 8},
        columns/Criterion/.style ={string type,column type=c, column name=Optim.},
		columns/DocRep/.style ={string type,column type=c, column name=Features},
        columns/SimMeas/.style ={string type,column type=c, column name=Similarity},
		columns/0/.style ={column type=c, column name=Voc.},
		columns/1/.style ={column type=c, column name=\textit{f}},
		columns/2/.style ={column type=c, column name=$\sigma$},
        columns/3/.style ={column type=c, column name=\textit{I}},
		columns/4/.style ={column type=c, column name=Prec.},
		columns/5/.style ={column type=c, column name=Rec.},
		columns/6/.style ={column type=c, column name=AUC},
        columns/7/.style ={column type=c, column name=$F_{0.5}$},
        columns/8/.style ={column type=c, column name=$F_{1}$},
		]{tables_data/AUC_FStatistics_tables/RFSE_SANTINIS_Best.csv}
\caption{Best performing models for RFSE on SANTINIS corpus.}
\label{chap:noise:tbl:RFSE_SANTINIS}
\end{table}


\subsection{WGI with Structured Noise}
\label{chap:noise:sec:openness_evaluation}

In this section the RFSE and OCSVME algorithms we describe experiments using a corpus with structured noise. The KI-04 corpus has been used for this set of experiments. 

The experiments are extensively testing the algorithms' noise tolerance in the open-set classification task for different openness levels as explained in section \ref{chap:eval_methods:sec:openness}. In more detail, the openness measure is adopted varying the number of training classes from 7 to 1 while keeping the number of testing classes always the same, at maximum 8. As a result, the openness measure varies from 0.065 to 0.646. 

One extreme refers to the case where only one genre class is unknown while in the other extreme only one genre class is known. In the extreme case of the maximum openness level, the problem is actually reduced to a binary problem of 1-vs-rest. On the contrary, in the extreme case of minimum openness level, the problem is a multi-class classification with only one unknown class which is virtually complete, i.e. contains single genre pages and no other pages that could be considered as noise.

The known classes are randomly selected for each openness level and the experiment are repeated 8 times, where each time performing 10-fold cross-validation. Moreover, to avoid any biased selection of parameter values, the parameter settings found to be optimal for the SANTINIS corpus are used, in section \ref{chap:noise:sec:WGI_noise}.

Figures \ref{chap:noise:fig:OCSVME_openness_test} and \ref{chap:noise:fig:RFSE_openness_test} show the performance ($F_{1}$) of OCSVE and RFSE models using different text representation features for varying openness levels. Standard error bars are also depicted to show the variance of performance for each model. 

RFSE models based on C4G and W1G gradually get worse while openness increasing while W3G models seems to be relatively stable. Surprisingly, the performance of OCSVM seems to improve by increasing openness and this pattern is consistent in all three feature types while C4G seem to be the most effective type. Although, in the maximum openness level the problem is equivalent to the closed-set binary (i.e. 1-vs-rest) classification problem.

\begin{figure}[H]
\begin{center}
    \includegraphics[scale=0.45]{Figures/OCSVME_openness_test_graph.eps}
	\caption{OCSVM performance in varying openness level.}
	\label{chap:noise:fig:OCSVME_openness_test}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
    \includegraphics[scale=0.45]{Figures/RFSE_MIX_openness_test_graph.eps}
	\caption{RFSE performance in varying openness level.}
	\label{chap:noise:fig:RFSE_openness_test}
\end{center}
\end{figure}


As it was highlighted in the previous section, according to the properties of the application in which WGI is involved, precision may be more important than recall or vice-versa. In figure \ref{chap:noise:fig:RFSE_precision_focus_openness_test} the macro-precision of RFSE is depicted for W3G, W1G and C4G features. MinMax similarity is used since it increases significantly the performance of RFSE in respect with precision. As concerns text representation, W1G is the best choice when precision is at more importance than recall. On the other hand, W3G features seem to be more stable because the standard error is lower than that of the other features and also the W3G model is not affected too much when openness surpasses $0.5$ (actually it improves).

\begin{figure}[H]
\begin{center}
    \includegraphics[scale=0.45]{Figures/RFSE_Precision_Focus_openness_test_graph.eps}
	\caption{RFSE precision in varying openness level.}
	\label{chap:noise:fig:RFSE_precision_focus_openness_test}
\end{center}
\end{figure}

In the case of C4G and W1G where the openness level is $0.646$ the standard error in both case is high. Since, this problem is only occurring in the case where the problems has been reduced to binary, it is interesting to see whether it is caused by choice of the document representation or by the choice of the similarity measure.

%In figures \ref{chap:noise:fig:RFSE_MIXvsMinMax_W3GvsC4G_openness_test} the $F_{1}$ measure performance in %the openness test of the RFSE is depicted. In all three cases we see the only with MinMax %similarity the standard error is significantly high especially in the case of $0.646$ openness %level.

Despite OCSVM's improvement when structured noise is used, it can only be competitive to RFSE on a high openness level, where all genre labels but one are considered unknown. This can be better viewed in figure \ref{chap:noise:fig:RFSE_vs_OCSVME_W1G_openness_test} where OCSVM is compared with RFSE models based on MinMax and Combo similarity measures for a varying openness level. These curves correspond to W1G features, so they are not the optimal models. However, they provide a fair comparison between examined methods. As standard error bars indicate, the performance of RFSE models with respect to the $F_{1}$ measure is significantly better than that of OCSVM while openness is less than $0.5$. Beyond that level, OCSVM is significantly better than RFSE models. It should also be noted that Combo measure helps RFSE in while openness is relatively low and MinMax seems to be a better choice when openness increases.

\begin{figure}[H]
\begin{center}
    \includegraphics[scale=0.45]{Figures/RFSE_vs_OCSVME_W1G.eps}
	\caption{Comparison of OCSVM and RFSE models based on W1G features in varying Openness levels.}
	\label{chap:noise:fig:RFSE_vs_OCSVME_W1G_openness_test}
\end{center}
\end{figure}

\section{Conclusions}\label{chap:noise:sec:conclusions}

In this chapter it has been presented an experimental study on WGI focusing on open-set evaluation for this task. In contrast to vast majority of previous work in this area, the open-set scenario is adopted which is more realistic for WGI, since it is not feasible to construct a genre palette with all available genres and appropriate samples for each one of them. Moreover, we examined two open-set classification methods and several feature types and similarity measures.

The presented evaluation of open-set WGI covers two basic scenarios. The first is when noise is unstructured, i.e., information about the true genre of pages not belonging to the known genre palette is not available. The second scenario applies when noise is structured, i.e., we actually know the true genre of pages not included in the training classes. For both cases they have been used the proposed appropriate evaluation methodologies for the open-set classification, presented in chapter \ref{chap:eval_methods}.

In almost all examined cases, RFSE models outperformed the corresponding OCSVM models. This verifies previous work findings about the appropriateness of RFSE for WGI \parencite{pritsos2013open}. RFSE is able to provide effective models and additionally it is possible to manage preference on recall or precision, an application-dependent choice, by focusing on optimizing AUC or $F_1$ respectively. On the other hand, OCSVM proved to be the best-performing method in extreme cases when openness is high. Actually, the restrictions of the available corpora did not allow us to examine cases where openness approaches $1.0$. However, it seems that when openness is more than $0.5$ OCSVM outperforms RFSE.

As concerns the feature types, in most of the cases W3G and C4G provided the best results. However, the selection of text representation features is a crucial choice that affects performance and it seems to be corpus-dependent. Another crucial parameter of RFSE is the similarity measure. Among the examined measures, MinMax and its combination with cosine similarity provide the most robust results. The choice of similarity measure correlates with feature types. It seems that the combo measure is more effective than MinMax in low openness conditions.

To enhance the evaluation of WGI models in open-set conditions, we need larger corpora including multiple genre labels. New enhanced open-set WGI methods are needed and they should be evaluated using the proposed paradigm. Otherwise, using an evaluation paradigm more appropriate for closed-set tasks, the performance may be over-estimated.