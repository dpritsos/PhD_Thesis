%!TeX spellcheck = en-US

\chapter{Introduction}

\label{chap:introduction}

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{Text Mining} \label{chap:introduction:sec:text_mining}

\textit{Text mining} roughly concerns knowledge discovery in texts, i.e. the process where \textit{Information Retrieval} (IR), \textit{Natural Language Processing} (NLP), and \textit{Machine Learning} (ML) methods are used for extracting \textit{high-level} information from texts. This information could refer to thematic/opinion/stylistic analysis of texts \parencite{hotho2005}. Given the huge amount of texts in electronic form produced daily in Internet media, this general research field has many applications in diverse areas including business and marketing, digital humanities and cyber-security \parencite{Weiss2010}. 

The main tasks in text mining research are following \parencite{Aggarwal2012}:

\begin{itemize}
\item \textit{Text Retrieval}: Given a large repository of documents, the goal is to enable easy access to the stored information by retrieving the subset of documents matching the information need of a user. A typical example is web search engines.
\item \textit{Information Extraction}: The goal is to extract specific information from documents, e.g. the names of people/places/organizations and dates of events in news stories.
\item \textit{Text Classification}: The goal is to assign labels from a predefined set to documents. Such labels could correspond to thematic area (e.g., 'politics', 'sport'), or the sentiment of texts (opinion mining) or the author of documents.
\item \textit{Text Clustering}: The goal is to group documents according to their similarity. This is used when there is no predefined list of categories and can also create structured taxonomies that organize and facilitate access to a document collection.
\item \textit{Text Visualization}: This aims at graphically depicting the main information found in a collection of documents to facilitate the exploration of similarities/differences among them and provide understandable information.
\item \textit{Document Summarization}: The goal is to provide a brief summary of a long document or a collection of documents by removing trivial details and including all crucial information. This facilitates access to collections of documents that are constantly updating. 
\end{itemize}

\section{Classifying Documents by Genre} \label{chap:introduction:sec:classifying_by_genre}

\textit{Genre Identification} is the natural progress of the almost ancient process of categorizing the human intellectual creations on such an abstract taxonomy as their Genus. Artifacts such as paintings, music pieces and written texts are always a subject of research interest to be classified based on their from, style and communicative purpose rather than their content. For example, novels or poems for documents, impressionism or expressionism for paintings, blues or funky for music, are some examples of genres that depend on structural information. Especially for documents, the defining factors for distinguishing between genres are their form, style, and communicative purpose.

There is a great debate for defining the notion of genre in the linguistic studies. Additionally, the genre notion is confusing when compared with other abstract categorizations of texts such as the \textit{text types} or \textit{registers} etc. Despite the methodological differences the linguistic community concluded that the idiosyncrasy of the genre taxonomy is mutable and diverse \parencite{coutinho2009describe}. This kind of idiosyncrasy is yielded to the genre taxonomy due to the spontaneous genesis of the genre classes. Genre classes are emerging or mutating when a communication process is taking place.

%\theoremstyle{definition}
\begin{definition}{\textit{Genre}}
is the genus of some arbitrary texts, which comprehensively describes their \textit{form, style} and \textit{communicative purpose} other than their content, where it emerges as a sociocentric interaction for accelerating the social communication when it comes to the description of the texts.
\end{definition}

\textit{Automated Genre Identification (AGI)}: Identification of the text's genre and sometime equivalent to text's register. That is the the automated identification of the form, style and communicative purpose of texts. \textit{News} indicates a different kind of texts than \textit{Blogs} with respect to genre. \textit{Editorial} is different than \textit{Article} with respect to the register while both can be considered as opinion articles written in argumentative style.

A subset of AGI is \textit{Web Genre Identification (WGI)} focusing on the World Wide Web where enriched documents (hypertexts) are classified on a given genre taxonomy/palette (e.g., blogs, home pages, e-shops, discussion forums, etc). The ability to automatically recognize the genre of web documents can enhance performance in several applications including the following:

\begin{itemize}
    \item IR systems can enable genre-based grouping/filtering of search results \parencite{Braslavski2007,Rosso2008}. A search engine can provide its users the option to define sophisticated queries combining genre labels and topics (e.g., blogs about machine learning or e-shops about sports equipment).
    \item Specialized collections and intuitive hierarchies of web page collections can be built by combining topic and genre information \parencite{de2009genre}. Genre-aware focused crawling, unlike general web-crawling, explores and downloads only relevant web-pages belonging to certain genres \parencite{deAssis:2017}. As a result valuable time and resources are saved and more specialized indices can be produced. The main challenge in this task is to be able to guess the genre of web-pages in advance, i.e. before the page is actually downloaded \parencite{priyatam2013don_URL}. 
    \item Knowing the genre of web-pages can be very helpful information in order to assess their credibility in spam detection \parencite{Agrawal:2018}. 
    \item In cyber-security, genre of web-pages can be exploited to enhance anti-phishing attempts \parencite{Abbasi:2015}.
    \item The recognition of web genre can also enhance the effectiveness of processing the content of web pages in information extraction applications. For example, given that a set of web pages has to be part-of-speech tagged, appropriate models can be applied to each web page according to their genre \parencite{Nooralahzadeh2014}.
\end{itemize}

Despite such interesting application areas, research in WGI is relatively limited due to fundamental difficulties emerging from the genre notion itself. The most significant difficulties in the WGI domain are the following:

\begin{itemize}
\item There is not a consensus on the exact definition of genre \parencite{crowston2011problems}. 
\item There is not a common genre palette that comprises all available genres and sub-genres \parencite{santini2011cross,mehler2010genres_on_web,mason2009n,sharoff2010web}, moreover, genres are evolving in time since new genres are born or existing genres are modified \parencite{Boese2005}. 
\item It is not clear whether a whole web page should belong to a genre or sections of the same web page can belong to different genres \parencite{jebari2015combination,madjarov2015web}. 
\item Style of documents is affected by both genre-related choices and author-related choices \parencite{petrenz2011stable,Sharroff2010}. As a result, it is hard to accurately distinguish between personal style characteristics and genre properties when style is quantified.
\end{enumerate}

\section{Closed-set vs. Open-set Classification} \label{chap:introduction:sec:openset}

In a typical text classification task, we are given a collection of documents $\mathcal{D}=\{d_1,\dots,d_{|\mathcal{D}|}\}$ and a set of labels $\mathcal{C}= \{c_1, \dots c_{|\mathcal{C}|} \}$ and the task is to assign each document to some of the labels. That is, for each pair $<d_j,c_i>\in \mathcal{D}\times \mathcal{C}$ a binary answer is produced indicating whether document $d_i$ is assigned to class $c_j$. Usually, text classification tasks are successfully handled by applying supervised machine learning methods \parencite{sebastiani2002}. This assumes the availability of a labeled training corpus $\mathcal{T}$ = $\{d_1,\dots, d_{|\mathcal{T}|}\} \subset \mathcal{D}$ where every pair $<d_j,c_i>$ is either a positive or a negative instance of $c_i$. Then, a classifier learns a function $\phi$:$\mathcal{T}\times\mathcal{C}\rightarrow \{True, False\}$ that approximates the target function $\check{\phi}$:$\mathcal{D}\times\mathcal{C}\rightarrow \{True, False\}$. The effectiveness of the classifier is estimated using another labeled dataset (test/evaluation set) $\mathcal{E}$ = $\{d_1,\dots, d_{|\mathcal{T}|}\} \subset \mathcal{D}$ that is non-overlapping with the training set.

Most previous studies in WGI consider the simple case where all web pages should belong to a predefined taxonomy of genres \parencite{Lim2005,santini2007automatic,kanaris2009learning,jebari2014pureURL}. This is known as closed-set classification.

%\theoremstyle{definition}
\begin{definition}{\textit{Closed-set Classification}}
assumes that the training and test sets are drawn from the same distribution and all their instances necessarily belong to at least one of the predefined labels. 
\end{definition}

There are several variations of that scenario, for example single-label (where each web-page belongs to exactly one label) or multi-label classification (where it is possible multiple labels to be assigned to a certain web-page), and soft classification (where an algorithm can return the probability score for every class from the trained label space \parencite{geng2018recent}).

The naive assumption of closed-set classification is not appropriate for most applications related with WGI. As already mentioned, it is not feasible to define a complete set of web genres. The scale of the Web makes any attempt to map existing web-pages to a specific genre label intractable. In addition, web genres in particular are evolving in time, some are modified or seize to exist and new ones are emerging (e.g., some years ago, blogs or tweets were unknown). The vast majority of previous work in WGI avoid to consider such concerns and as a result their effectiveness in closed-set classification conditions is over-estimated.

It is therefore realistic to assume that despite best efforts to define a long genre label list, there will always be a great amount of web-pages that do not belong to any of these. Previous work in WGI define such web-pages as \textit{noise} (this term can also refer to the case where multiple genres co-exist and there is no dominant genre label) \parencite{santini2011cross,levering2008using}. To handle noise in WGI there are two main options. First, to adopt the closed-set classification setup having one predefined category devoted to noise. Positive training examples are given for this noise class. Since this category would comprise all web pages not belonging to the known genre labels, it would not be homogeneous and it is not clear how to sample it. Moreover, this noise class would be much more greater with respect to the other genres causing class imbalance problems. 

The second option is to adopt the open-set classification setting where it is possible for some web pages not to be classified into any of the predefined genre categories \parencite{stubbe2007genre,pritsos2013open}. This setup avoids the problem of class imbalance caused by numerous noisy pages and also avoids the problem of handling a diverse and highly heterogeneous class. On the other hand, open-set classification requires strong generalization with respect to the closed-set setup \parencite{scheirer2013toward}.

%\theoremstyle{definition}
\begin{definition}{\textit{Open-set Classification}}
assumes that it is likely for samples of classes unseen during the training phase to appear in test phase. An open-set classifier should be able to accurately recognize test instances belonging to the \textit{known} classes (seen during training) and avoid to be confused by instances belonging to unknown classes (not seen during training) \parencite{geng2018recent}.
\end{definition}

Open-set classification is closely related to the \textit{Novelty Detection} and \textit{One-class Classification} where it is assumed that only positive examples of a particular class are available for the supervised learning methods. These methods have been adapted to this problem and there are several examples such as One-Class SVM, One-Class Neural Networks, etc. It might sound similar but it is not a binary classification setup for training these algorithms due to the lack of the negative examples. One-class classification requires very strong generalization and it is suitable when either the negative class is not available or it is huge and heterogeneous so that it is not possible to be adequately sampled. 

It is possible to transform a (soft) closed-set classifier to an open-set one by introducing a \textit{reject option} that is used to leave a test instance unclassified. For example, a reject option may examine how far a test instance is from the class centroids or what the difference in decision probabilities between the most likely classes is and in case some predefined criteria are not met then the test instance is left unclassified \parencite{onan2018ensemble}. Closed-set classification methods with a reject option are not open-set essentially since they avoid to estimate the \textit{open-space risk}.

Each classifier attempts to draw boundaries between the known classes (i.e., seen during training phase). A closed-set classifier (no matter if it uses a reject option) separates the whole instance space by such decision boundaries. However, the samples of known classes may be gathered in specific parts of the instance space. The space far away from known class instances is known as the \textit{open space}. The open-space risk refers to the act of labeling a test instance in the open-space \parencite{geng2018recent}. 

A more formal definition of open-set classification is one where the open space risk is considered. Let $\mathcal{T}$ be the training data, $R_{O}$ the open space risk, and $R_{\epsilon}$ the empirical risk. Then the objective of open-set classification is to find a function $f \in L$ which minimizes the following \textit{open-set risk}: 

\begin{equation}
\argminA_{f} \{R_{O}(f ) + \lambda R_{\epsilon}(f (\mathcal{T}))\}
\end{equation}

\noindent where $f (x) > 0$ implies correct recognition and $\lambda$ is a regularization constant. Thus, open-set risk balances the empirical risk and the open space risk \parencite{geng2018recent}. In practice the empirical risk is the loss function of the open-set classification model in the training set while the open-space risk is the ratio of the open space to the full vector space.

\section{Representation of Web-pages} \label{chap:introduction:sec:document_representation}

In order to use supervised learning technology to WGI, it is required to transform the information in raw web documents into a quantitative representation. This means that each web-page should be represented as a numerical vector where each dimension (feature) properly captures relevant information. In addition, ideally the vectors should be dense and compact to enable ML algorithms deal with the classification task efficiently.

The web documents can be considered a super-set of the document format types because it expands Postscript \footnote{Postscript is the digital format used from the Desktop Publishing (e.g. PDF or PS formats). In this thesis this term is used to describe all traditional document formats such as books, magazines, newspapers, in contrast to the enriched (hyperlinked) web documents.} by introducing functionality and versatility based on HTML and virtually infinite inter-connectivity because of the hyperlinks. 

In relevant literature there is a great variety of ideas aiming at document representation for WGI. The main features that can be extracted from web-pages are related to the following information:

\begin{enumerate}
\item The Uniform Resource Locator (URL) and hyperlinks of web-pages (and the graph formed by these connections).
\item The HTML tags and Document Object Model (DOM) structure of the web-page. 
\item The textual content of the web-page.
\end{enumerate}

In some cases, it has been reported that the web-pages's URL alone is sufficient for predicting its genre \parencite{abramson2012_URL,jebari2014pureURL,priyatam2013don_URL,zhu2011enhance}. Concerning available hyperlinks in web-pages there are two parts than can provide useful information: the URL of the hyperlink itself handled as a string of characters and its \textit{anchor text}. Alternatively, the structure of the graph which is formed by the hyperlinks and information found in neighboring pages can also be used. Usually, the neighbouring pages can contribute by amplifying the signals for the correct genre classification using either information extracted from their text or based on the assumption that pages of the same genre tend to be inter-linked \parencite{abramson2012_URL,asheghi2014semi,jebari2014pureURL,priyatam2013don_URL,zhu2011enhance}.

The HTML tags can provide useful information about the structure of web-pages. In the simplest approach, HTML tags can be treated as raw text and the frequency of specific tags is measured with some potential heuristics. However, the W3C suggested HTML web-page composition paradigm is changing and constantly violated. As a result, heuristics can only contribute but in a few practical cases. A more sophisticated and sensible approach can be the analysis of the DOM structure, where the format of the text can be captured. As an example, e-shop web-pages are different from the academic web-pages. This resembles the difference in typographic format of a printed magazine and a printed newspaper. However, most likely several heuristics are needed for identifying these structures, because of the HTML composition paradigm violation \parencite{mehler2011integrating}.

The bulk of research work in WGI has focused mostly on the features which can be extracted from the textual part of web-pages (i.e., after the removal of HTML tags) \parencite{mason2009classifying,sharoff2010web,Sharroff2010,Nooralahzadeh2014,onan2018ensemble}. The following are the main categories of textual features: 

\begin{enumerate}
\item Lexical features: Each web-page is seen as a series of tokens and frequencies of specific words (e.g. function words) or sequences of tokens (e.g., word n-grams) can be measured. In addition, information about the length of words and sentences can be useful.
\item Character features: Each web-page is handled as a alphanumeric string and usually frequencies of character n-grams can provide a very detailed and highly dimensional representation. 
\item Syntactic features: This requires some kind of sophisticated analysis by NLP tools that can provide information about the syntactic patterns found in the web-pages. One popular and relatively simple approach is the use of part-of-speech (POS) n-grams. Syntactic features are language-dependent and their reliability correlates with the error rate of the used NLP tools.
\end{enumerate}

Typical term weighting schemes, like binary, Term Frequency (TF) and Term Frequency - Inverted Document Frequency (TF-IDF) are popular in WGI. In addition, there are some schemes specifically designed for WGI tasks like  \textit{Term Frequency - Inverted Genre Frequency} (TF-IGF). This is an extension of TF-IDF that is based on the frequencies of a term in the documents of particular genre rather than the whole corpus \parencite{sugiyanto2014term,}.

Recently, \textit{distributed representations} provide an alternative way to represent documents using neural network language models \parencite{mikolov2013distributed,le2014distributed}. In contrast to the popular n-gram features that produce sparse vectors, distributed representations produce dense vectors of relatively low dimensionality. This approach has obtained state-of-the-art effectiveness in several text classification tasks but it has not thoroughly tested in WGI so far. 

\section{Motivation} 
\label{chap:introduction:sec:motivation_objective}

As already mentioned, the vast majority of previous work in WGI adopt the closed-set classification scenario that is not realistic and leads to an over-estimation of performance. Since it is not feasible to define a complete list of genre labels and genres constantly evolve in time, the open-set classification scenario better suits WGI. 

Among the few attempts to follow open-set classification in WGI, very few use pure open-set classifiers \parencite{stubbe2007genre,Asheghi2015}. An additional issue is how to handle the test web-pages belonging to unknown genres. One option is to consider these as \textit{unstructured noise} where the true genre of noisy pages is not available and another is to examine \textit{structured noise} where the true genre of noisy pages is available (yet unknown during the training phase). 

So far, it is not clear what specific open-set classification methods can better handle these cases. In addition, there is lack of a evaluation framework that can appropriately measure the effectiveness of open-set WGI methods with the presence of either unstructured or structured noise. This requires the use of appropriately defined evaluation measures and the suitable design of experimental setup. In addition, we need a clear way to compare different methods in application-dependent conditions where, for example, precision may be considered more important than recall.

Most previous studies attempt to combine heterogeneous information coming from the hyperlinks between web-pages, the HTML code and the textual content of web-pages. Despite the usefulness of all these information, the main question is whether it is possible to accurately predict the genre of a web-page focusing on its textual content since this is not affected by technology changes and habits of web developers or arbitrary changes in neighboring web-pages. 

There is a great variety of text representation measures applied to WGI, most of them attempt to capture the stylistic properties of web genres. It is not yet clear how specific approaches, like word and character n-grams, known to be very effective in closed-set WGI \parencite{sharoff2010web}, are still effective in open-set WGI where the dimensionality of the representation may severely affect the ability of the open-set classifier for  generalization.

Finally, the recent success of the use of distributed representations acquired by neural network language models in other text classification tasks is a strong motivation to attempt to examine their effectiveness also in open-set WGI. One main advantage of such approaches is that they produce a space of relatively low dimensionality and in theory this may be an advantage for specific open-set classifiers that may suffer when irrelevant and redundant features are available.

\section{Contribution} \label{chap:introduction:sec:contribution}

This thesis focuses on open-set WGI and examines specific algorithms and experimental setups that allow their evaluation in realistic conditions. More specifically, the main contributions are listed bellow:

\begin{itemize}
\item An approach based on one-class classification, where only positive training examples of a target class are considered, is introduced to WGI. The proposed method is based on \textit{one-class support vector machines} (OCSVM) and is modified to handle multi-class open-set classification. This algorithm is presented in detail in section \ref{chap:openset:sec:OCSVM_description}.
\item The \textit{Random Feature Subspacing Ensemble} (RFSE) is introduced to WGI. This open-set classifier is based on an existing approach originally proposed for authorship attribution and it is adopted to better handle the WGI task \parencite{koppel2011authorship}. This algorithm has been implemented in python and in its general form can handle any kind of text representation\footnote{https://github.com/dpritsos/RFSE}. This algorithm is presented in detail in section \ref{chap:openset:sec:RFSE_Description}.
\item Another open-set classifier, the \textit{Nearest Neighbors Distance Ratio} (NNDR) is introduced to WGI. This is a modification of the well-known k-Nearest Neighbor classifier \parencite{mendesjunior2016} and it is extended to better suit the WGI requirements. This algorithm has been implemented in python\footnote{https://github.com/dpritsos/OpenNNDR} and is presented in detail in section \ref{chap:openset:sec:NNRD_Description}.
\item The noise (i.e., web-pages not belonging to any of the known genres) in WGI is distinguished into \textit{unstructured} and \textit{structured} noise and each case is thoroughly studied. The former considers all unknown genres as a common heterogeneous class. The latter admits that there is structure in the unknown web-pages, namely the existence of genre labels not seen during the training phase. In this thesis it is introduced the \textit{openness} as an indication of how the number of known classes is compared to the number of unknown classes. This concept is borrowed by relevant work in visual object recognition \parencite{scheirer2013toward} and it perfectly suits the WGI task.
\item An experimental framework suitable for evaluating open-set WGI algorithms is introduced including abilities to study different kinds of noise (unstructured or structured). The use of openess enables the study of open-set WGI where the difficulty of the task is explicitly controlled (i.e., few known classes vs. many unknown classes or many known classes vs. few unknown classes). In addition, appropriate evaluation measures provide a detailed view on the obtained performance. This is especially important since evaluation measures usually involved in closed-set classification can be misleading since they handle all classes equally. However, in open-set WGI, the class of unknown web-pages (including all web-pages that do not belong to known genres) is usually much larger than the known classes and it should be treated in a special way as it is explained in Chapter \ref{chap:eval_methods}. 
\item The proposed open-set WGI algorithms are extensively evaluated using the aforementioned experimentation framework. The particular hyper-parameters and settings that allow these algorithms to achieve as good results as possible are examined. In addition, the use of different kinds of text representation is considered and their effect on the performance of each algorithm is studied. The most popular textual features in WGI covering lexical, character, and syntactic features are considered.
\item The application of distributed representations acquired from neural network language models in WGI is explored. The effect of such low dimensional and dense representations on the effectiveness of the NNDR open-set WGI algorithms is studied. It is demonstrated that especially the precision of this approach can be considerably enhanced making it more suitable for specific WGI applications.

%\item \textbf{The Genre-units} are, also, discussed in this study such as \textit{the web-page, the web-page section, the web-page paragraph} or \textit{the web-site multi-genres}. Consequently, the URL utility in the WGI task is raised and discussed in respect of the linking of these units and how it can be used as an indicator of the genre-identification. Then noise notion is changing slightly but the same approach can be applied.

%\item In order to pre-process the HTML raw web-pages of the corpora used for the experiments of this thesis a specialized tool has been implemented called \textbf{Html2Vec}\footnote{https://github.com/dpritsos/html2vec}. This has a well designed API in order to be rapid expandable for handling any HTML heuristics and return any kind of Vectors required for a specific experiment. It handles special HTML characters, is cleaning or extracting the HTML elements. It can also recognize the \textit{Numbers, IP addresses, URLs, Currency Numbers} for reducing the noise might be created in the \textit{dot (,)} and \textit{space ( )} will be used as separation characters for the terms extraction form the text. Moreover, in can return the TF, and Word2Vec vectors of a corpus.
\end{itemize}

\section{Publications}

Parts of the work described in this thesis have already been published in scientific journals and conference proceedings. The list of related publications is following:

\begin{itemize}
\item D.A. Pritsos, and E. Stamatatos, Open-set Classification for Automated Genre Identification, In \textit{Proc. of the European Conference on Information Retrieval} (ECIR 2019), pp. 207-217, LNCS 7814, Springer, 2013.
\item D. Pritsos and E. Stamatatos, The Impact of Noise in Web Genre Identification, In \textit{Proc. of the International Conference of the Cross-Language Evaluation Forum for European Languages} (CLEF 2015), pp. 268-273, LNCS  9283, Springer, 2015.
\item D. Pritsos and E. Stamatatos, Open Set Evaluation of Web Genre Identification, \textit{Language Resources and Evaluation}, 52(4), pp. 949-968, Springer, 2018.
\item D. Pritsos, A. Rocha, and E. Stamatatos, Open-Set Web Genre Identification Using Distributional Features and Nearest Neighbors Distance Ratio, In \textit{Proc. of the European Conference on Information Retrieval} (ECIR 2013), pp. 3-11, LNCS 11438, Springer, 2019.
\end{itemize}

\section{Thesis Outline} \label{chap:Introduction:sec:thesis_outline}

The rest of this thesis is outlined below. 

Chapter \ref{chap:relevant_work} discusses relevant work on AGI and WGI tasks. Definitions and uses of genre from the fields of linguistics and computational linguistics are presented. The state-of-the art for the representation of web-pages and the ML methodologies for genre identification are discussed. The few open-set WGI approaches are described. Finally, the available corpora for evaluating WGI methods and their properties are discussed.

Chapter \ref{chap:openset} focuses on open-set WGI and analytically presents the three algorithms examined in this thesis (i.e., OCSVM, RFSE, and NNDR). The characteristics of these methods and their differences of with existing approaches are discussed.

Chapter \ref{chap:eval_methods} introduces the experimental framework proposed in this thesis for evaluating open-set WGI approaches. The use of openess as a means to control the difficulty of WGI tasks is discussed. Appropriate evaluation measures are defined for both unstructured and structured noise. 

Chapter \ref{chap:noise} deals with the experimental analysis of OCSVM and RFSE algorithms. The evaluation corpora used in this study and their properties are discussed. Experiments when structured and unstructured noise is considered are presented. The effect of text representation on the effectiveness of the examined methods is studied.

In Chapter \ref{chap:word_embeddings}, the usefulness of distributed representation in open-set WGI is presented. The NNDR algorithm is evaluated using traditional n-gram-based features and distributed features. Experimental results show how the performance of this algorithm is affected and it compares with OCSVM and RFSE. 

Finally, Chapter \ref{chap:conclusions} summarizes the main conclusions drawn from this study and discusses future work directions. 

