%!TeX spellcheck = en-US

%\chapter{Evaluation Methodology for WGI and Computational Text Categorization}
\chapter{An Evaluation Framework for Open-set WGI}

\label{chap:eval_methods}

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

The table \ref{chap:eval_methods:tbl:bin_confusion} has $140$ total samples which is equal either the the row-sums or the column-sums are summed up. In the case of open-set classification some of the samples are remaining as non-classification or as \textit{unknown} where there is no class trained for them. 

In table \ref{chap:eval_methods:tbl:multi_confusion} there are still 140 sample distributed in a seven (7) classes and also with 42 of them remaining as unclassified. Given these cases, the respective per class, macro and micro recall for this confusion matrix are calculated and shown in table \ref{chap:eval_methods:tbl:macro_vs_micro}.

\begin{table}[t]
	\center
	\caption{Confusion metric of binary classification}\label{chap:eval_methods:tbl:multi_confusion}
	\begin{tabular}{c c c c c c c c c c c}
		& & \multicolumn{8}{c}{Actual} & \\
		\cline{3-10}
		\multirow{10}{*}{\rotatebox[origin=c]{90}{Predicted}} & & \multicolumn{1}{|c}{A} & \multicolumn{1}{c}{B} & \multicolumn{1}{c}{C} & \multicolumn{1}{c}{D} & \multicolumn{1}{c}{E}  & \multicolumn{1}{c}{F} & \multicolumn{1}{c}{G} & \multicolumn{1}{c|}{\emptyset} & \\
		\cline{2-10}
		& \multicolumn{1}{|c}{A} & \multicolumn{1}{|c}{\textbf{13}} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c|}{0} & 15 \\
		& \multicolumn{1}{|c}{B} & \multicolumn{1}{|c}{1} & \multicolumn{1}{c}{\textbf{10}} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{3} & \multicolumn{1}{c|}{0} & 16 \\
		& \multicolumn{1}{|c}{C} & \multicolumn{1}{|c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{\textbf{1}} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{3} & \multicolumn{1}{c|}{0} & 4 \\
		& \multicolumn{1}{|c}{D} & \multicolumn{1}{|c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{\textbf{8}} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1} & \multicolumn{1}{c|}{0} & 10 \\
		& \multicolumn{1}{|c}{E} & \multicolumn{1}{|c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{\textbf{20}} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{1} & \multicolumn{1}{c|}{0} & 25 \\
		& \multicolumn{1}{|c}{F} & \multicolumn{1}{|c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{\textbf{10}} & \multicolumn{1}{c}{0} & \multicolumn{1}{c|}{0} & 13 \\
		& \multicolumn{1}{|c}{G} & \multicolumn{1}{|c}{0} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{\textbf{8}} & \multicolumn{1}{c|}{0} & 15 \\
		& \multicolumn{1}{|c}{\emptyset} & \multicolumn{1}{|c}{\textit{6}} & \multicolumn{1}{c}{\textit{8}} & \multicolumn{1}{c}{\textit{9}} & \multicolumn{1}{c}{\textit{8}} & \multicolumn{1}{c}{\textit{0}} & \multicolumn{1}{c}{\textit{7}} & \multicolumn{1}{c}{\textit{4}} & \multicolumn{1}{c|}{0} & \textbf{42} \\
		\cline{2-10}
		% & & 0 & 14 & 12 & 11 & 12 & 20 & 13 & 16 & \textbf{70}\\
		% \cline{3-11}
		& & \textit{0} & \textit{20} & \textit{20} & \textit{20} & \textit{20} & \textit{20} & \textit{20} & \textit{20} & \textbf{70}\\
	\end{tabular}
\end{table}


\begin{table}[H][t]
	\center
	\caption{Macro and Micro calculation for the Confusion matrix (Table \ref{chap:eval_methods:tbl:multi_confusion}) of multi-class classification}\label{chap:eval_methods:tbl:macro_vs_micro}
	\begin{tabular}{l l l}
		\cline{2-3}
		& \multicolumn{1}{|c}{Precision} & \multicolumn{1}{c|}{Recall}\\
		\cline{1-3}
		\multicolumn{1}{|c}{A} & \multicolumn{1}{|c}{0.866} & \multicolumn{1}{c|}{0.650}\\
		\multicolumn{1}{|c}{B} & \multicolumn{1}{|c}{0.625} & \multicolumn{1}{c|}{0.500}\\
		\multicolumn{1}{|c}{C} & \multicolumn{1}{|c}{0.250} & \multicolumn{1}{c|}{0.050}\\
		\multicolumn{1}{|c}{D} & \multicolumn{1}{|c}{0.800} & \multicolumn{1}{c|}{0.400}\\
		\multicolumn{1}{|c}{E} & \multicolumn{1}{|c}{0.800} & \multicolumn{1}{c|}{1.000}\\
		\multicolumn{1}{|c}{F} & \multicolumn{1}{|c}{0.769} & \multicolumn{1}{c|}{0.500}\\
		\multicolumn{1}{|c}{G} & \multicolumn{1}{|c}{0.533} & \multicolumn{1}{c|}{0.400}\\
		\cline{1-3}
		\multicolumn{1}{|c}{Macro} & \multicolumn{1}{|c}{0.663} & \multicolumn{1}{c|}{0.500}\\
		\multicolumn{1}{|c}{Micro} & \multicolumn{1}{|c}{0.714} & \multicolumn{1}{c|}{0.500}\\
		\cline{1-3}
	\end{tabular}
\end{table}


Clearly the is a significant difference in the macro-P compare to the micro-P and for this case, micro and macro recalls are have both $0.500$ score. It should be noted that due to the 42 samples of the 140 that haven't been classified at all there is bias over precision score when its micro score is calculated. Moreover, there is problem in calculation of the recall based in the equation form \ref{chap:eval_methods:eq:recall} because in the closed set classification the denominator is equal to the total number pf samples that we know they are under the distribution of the class we are evaluating for. 

The recall calculation issue is cased because of the open-set framework and the sample are renaming out of the classification processing because of the rejection criterion of an open-set algorithm. In order to calculate the recall we are following the theoretical definition of the this score which is formally expressed in equation \ref{chap:eval_methods:eq:recall_theory}.

\begin{equation}\label{chap:eval_methods:eq:recall_theory}
	R = \frac {\text{The sum of correctly classified samples}} {\text{The total number of distribution's testing samples}}
\end{equation}

Thus in the case of \textit{micro-R} is the denominator is equal to the total number of all samples of the data-set. In the \textit{macro-R} case, is the number of the class-samples for every class separately and then the average score recall score of all classes.

In the table \ref{chap:eval_methods:tbl:multi_confusion} the micro recall show that only the $50\%$ of the total samples have been correctly classified and the $98/140 = 70\%$ of the total samples have been classified while the rest has been remained as \textit{unknown}. The macro recall is also $0.500$ based on the separated evaluation for every different class.

However, the precision is much more accurate in the case of macro compare to the micro because the micro precision is bias for the algorithms positive performance since the rejected samples (characterized as unknown) are not calculate in the precision performance or their are not causing any kind of penalty to the precision score. In addition the proper calculation of recall (micro and macro) as explained above will not differ in the extend of regulating the F-score.

The macro scores are then less biased for the the open-set framework evaluation. Moreover, in the highly imbalanced corpora also the macro scoring is reported to be more realistic and less biased. Due to the above reasoning the macro-P, macro-R, and Macro-F1 will mainly be the evaluation measure is used in this thesis.

In the next chapter the Precision Recall Curves will be presented where it is more vivid the effect of the micro and macro measurement. That is there it is clear the bias over the precision in the algorithm's performance because of the open-set algorithm criterion.


\subsection{Precision-Recall Curves}\label{chap:eval_methods:sec:roc_prc}

So far, the evaluation measures consider classification algorithms that provide crisp predictions (i.e., \textit{hard classifiers}). The discussed evaluation measures are only available to show particular aspects of the performance of classifiers. To obtain a deeper look we need richer evaluation methods that can depict the performance of classifiers in a variety of conditions. One such method is the \textit{Precision-Recall Curves (PRC)}, a standard method for evaluating information retrieval systems and ranking systems. They can only be applied to \textit{soft classifiers} that are able to explicitly estimate class conditional probabilities. Fortunately, the vast majority of hard classifiers can be adopted to also provide some form of score that can be regarded as class conditional probability.

The calculation of a PRC requires the ranking of estimated probabilities in descending order. In each step, the next prediction is considered and a new precision and recall point is calculated. Both macro-PRC and micro-PRC can be calculated.

In order to facilitate the comparison of PRCs corresponding to the performance of different systems on the same evaluation dataset, the 11-recall level normalization is typically used. The initial points of PRC are reduced to 11 that correspond to standard recall levels $[0,0.1,...,1.0]$. For example, in case Recall$=0.1$, we measure precision when 10\% of the samples have been seen. Precision values are interpolated based on the following formula:

\begin{equation}\label{chap:eval_methods:eq:11recall_level}
	P(r_j)=max_{r_j \leqslant r \leqslant r_{j+1}}(P(r))
\end{equation}

\noindent
where $P(r_j)$ is the precision at $r_j$ standard recall level ($r_j=\{0,0.1,0.2,...,1.0\}$).



\begin{table}[t]
	\center
	\caption{Macro and Micro calculation for the Confusion matrix (Table \ref{chap:eval_methods:tbl:multi_confusion}) of binary classification}\label{chap:eval_methods:tbl:prc}
	\begin{tabular}{|c|c|c|c|}
		\cline{1-4}
		Certainty & Predicted & Expected & Correct\\
		\cline{1-4}
		0.99 & 1 & 1 & 1 \\
		0.99 & 2 & 2 & 1 \\
		0.99 & 4 & 4 & 1 \\
		... & ... & ... & ... \\
		0.79 & 2 & 6 & 0 \\
		0.79 & 4 & 4 & 1 \\
		0.69 & 7 & 6 & 0 \\
		0.69 & 4 & 4 & 1 \\
		... & ... & ... & ... \\
		0.64 & \emptyset & 6 & 0 \\
		0.60 & 2 & 2 & 1 \\
		... & ... & ... & ... \\
		0.60 & \emptyset & 4 & 0 \\
		\cline{1-4}
	\end{tabular}
\end{table}

In table \ref{chap:eval_methods:tbl:prc} it is shown an sequence of calculation for the corpus formed the final confusion matrix of table \ref{chap:eval_methods:tbl:multi_confusion}. In this case, an open-set algorithm returned the predictions together with its certainty scores. 

\begin{figure}[t]
	\begin{center}
    	\includegraphics[scale=0.45]{Figures/pr_macro_micro_example.eps}
		\caption{Open-set Macro (Red line) and Micro (grey line) Precision Recall Curves of confusion matrix in table \ref{chap:eval_methods:tbl:multi_confusion}}
		\label{chap:eval_methods:fig:prc_macro}
	\end{center}
\end{figure}

The two curves yielding from this calculations, micro and macro PRC are shown in figure \ref{chap:eval_methods:fig:prc_macro}. Clearly, the micro-PRC is misleading compare to the macro-PRC and the results of table \ref{chap:eval_methods:tbl:macro_vs_micro}. 

The macro-P is measures (in table \ref{chap:eval_methods:tbl:macro_vs_micro}) are leading to the conclusion of a low performance algorithm, especially for the recall score. On the contrary the micro-P is leading to the conclusion of a high performance algorithm, which we know that it is not true, form the confusion matrix of \ref{chap:eval_methods:tbl:multi_confusion}.

To compensate the potentially unbalanced distribution of web pages over the genres, the macro-averaged precision and recall measures is used. In more detail, in this thesis a modified version of precision and recall is used, for open-set classification tasks proposed by \parencite{mendesjunior2016}. This modification calculates precision and recall only for the known classes (available in the training phase) while the unknown samples (belonging to classes not available during training) affect false positives and false negatives.

The problem of the misleading micro-PRC is cased by the presence of the outages, depicted in the first row, in the confusion matrix \ref{chap:eval_methods:tbl:multi_confusion}. Particularly, as shown in table \ref{chap:eval_methods:tbl:prc} and the rows 10 and 13, the micro-PRC calculation is considering the rejected incorrect predicted and it is including them in the prediction. Thus these predictions are not calculated in the precision equation \ref{chap:eval_methods:eq:precision}. These predictions are just decreasing the micro-R. Consequently, the curve is overestimating the performance of the algorithm.

On the contrary the macro-PRC is closer to the macro prediction and recall. Particularly it is show that the curve stops near the $0.5$ values which is equal to the macro-R. Moreover, macro-precision is high yet drops significantly due to the $0.60$. The PRC is giving the same evaluation performance with the table's \ref{chap:eval_methods:tbl:macro_vs_micro} scores per class and macro-scores. That is, the algorithm is having really good performance for some classes and really bad for some other.

However, due to the certainty ranking we can see more insight related to the algorithms performance. Particularly, we can see some irregular drops in the micro-PRC. Also the slope of the macor-PRC first recall level to the second is very small and then drops from the near perfect predictions to the $0.87$ precision etc. Also, the curve it stops at $0.5$ recall level.

These, Micro-PRC (and some time in Macro-PRC) 'irregular' properties are only meet at the open-set algorithms because of the rejection factor. In table \ref{chap:eval_methods:tbl:prc} the algorithms of the precision in rows 10 and 13, seems to have $0.60$ certainty factor which is almost as high as its correct predictions. These predictions are casing high fluctuations in the PRC which are regularized because the 11-recall level average normalization is applied on top. 


\section{Area Under the Curve (AUC)}\label{chap:eval_methods:sec:closed_set_classification} 

In table \ref{chap:eval_methods:tbl:AUC_F1} the Micro and Macro \textit{Area Under the Curve (AUC)} and F1 are presented. The AUC is a common values is used when several PRC's needs to be compared. As an example, to find parameter settings that obtain optimal evaluation performances for the open-set algorithms, the AUC for the PRC's of figure \ref{chap:eval_methods:fig:prc_macro} can be calculated. 


\begin{table}[t]
	\center
	\caption{Macro and Micro calculation for AUC and F1 of the Confusion matrix (Table \ref{chap:eval_methods:tbl:multi_confusion})}\label{chap:eval_methods:tbl:AUC_F1}
	\begin{tabular}{c|c|c|}
		\cline{2-3}
		& AUC & F1 \\
		\hline
		\multicolumn{1}{|c|}{Macro} & 0.448 & 0.570 \\
		\multicolumn{1}{|c|}{Micro} & 0.866 & 0.588 \\	
		\hline
	\end{tabular}
\end{table}

It is very important to note that the Micro-AUC is highly misleading, as expected from the PRC diagram \ref{chap:eval_methods:fig:prc_macro}. On the contrary the F1 is not so different, although, as explained, is overestimating the performance o the algorithm by overestimating the precision score. 

The effect of a this potential misleading choice, selecting the \textit{micro-scores} instead of the \textit{macro-scores}, is a bad choice of the  hyper-parameters required for the proper tuning of the classification algorithm. In the experiments (chapter \ref{chap:noise}) of this thesis, it will pointed out whenever this danger is occurring.

%\section{Re-defining the Open Space Risk}\label{chap:eval_methods:sec:open_space_risk} 

%The open space risk in \parencite{scheirer2013toward} is originally defined as in eq. \ref{chap:eval_methods:eq:the_original_open_space_risk}

%\begin{equation}\label{chap:eval_methods:eq:the_original_open_space_risk}
%	R_{o}(f) = \frac{\int_{o} f_{y}(x) dx}{\int{S_{o}}  f_{y}(x) dx}
%\end{equation}

%\noindent
%where $R_{o}(.)$ is the open-space risk function and $f_{y}(x)  \in \{0, 1\}$ is the classification function of class $y$, where $1$ is for recognizing its class and $0$ when not. $S_{o}$ is the large hyper-sphere where all the positive training data points and the \textit{positive open space area} $O$. 

%The original formulation of the eq. \ref{chap:eval_methods:eq:the_original_open_space_risk} $O$ area cannot be constrained by any means. The only information we are getting is the farther form the training date we go the risk of miss-classification is increasing. One method to constrain the problem is by using the center of the positively labeled training data and defining a radios $r_{o}$ where it will reduce the open space area based on the positively labeled empirically observations. Then the $O$ is defined by the equation eq. \ref{chap:eval_methods:eq:openspace_spherical_constrained}

%\begin{equation}\label{chap:eval_methods:eq:openspace_spherical_constrained}
%	O = S_{o} - B_{r_{y}}(C_{y})
%\end{equation}

%\noindent
%where $B_{r_{y}}(.)$ is the function which defines the area of radius $r_{y}$ of the $C_{y}$ class defined by its training data \parencite{fei2016breaking}.

\section{The Openness Test}\label{chap:eval_methods:sec:openness}

The open-set evaluation measures defined in this chapter can be used in both unstructured and structured noise. However, in the latter case, we need a more detailed analysis of the performance to indicate the ability of the open-set classifier to handle low/high number of training/unknown classes. It is especially important to study the relation of the number of training classes with respect to the number of unknown classes. 

In \parencite{scheirer2013toward}, the \textit{openness measure} is introduced to directly measure this relation. The openness measure indicates the difficulty of an open-set classification task by taking into account the number of \textit{training classes} (i.e. the known classes used in the training phase) and the number of \textit{testing classes} (i.e., both known and unknown classes used in the testing phase) \parentcite{mendesjunior2016}:

\begin{equation}\label{chap:eval_methods:eq:openness}
	openness=1-\sqrt{\frac{ | Training Classes | }{ |Testing Classes | }}
\end{equation}

When openness is $0.0$, it is essentially a closed-set task, that is the training and testing classes are exactly the same. This actually means that there is no noise. At the other extreme, when openness reaches $1.0$ this means that the known classes are far less than the unknown classes or that the amount of noise is especially high and heterogeneous. Therefore, by varying the openness level we can study the performance of WGI models in different conditions.

Note that the openness measure can only be applied to datasets where all available samples have been labeled with class information. In the case of WGI, we have to know the genre labels of the pages that form the noise (i.e. structured noise). This information is only used to quantify the homogeneity of the noise.

The study of open-set classifiers can be significantly extended by measuring their performance (e.g., $macro F_{1}$) for varying values of the openness score. Given that $\mathcal{U}$ is the set of unknown classes (structured noise) it is possible to vary the training classes from 1 to $|\mathcal{c}|$ while the testing classes can vary from $|\mathcal{C}|$ to $|\mathcal{C}|+|\mathcal{U}|$. 

\begin{figure}[t]
	\begin{center}
    	\includegraphics[scale=0.50]{Figures/openness_example.eps}
		\caption{An example of using the openness test. The smallest openness level corresponds to 6 training classes and 7 testing classes. The maximum openness level refers to 1 training class and 7 testing classes. At the 1.7 the the openness score is maximized for this open-set problem.}
		\label{chap:eval_methods:fig:openness}
	\end{center}
\end{figure}

In figure \ref{chap:eval_methods:fig:openness} the precision scores on different openness levels, are presented for evaluating three arbitrary open-set algorithms, on the corpus which it is forming the confusion matrix \ref{chap:eval_methods:tbl:multi_confusion}. Note, that the begging and the end of the curve are pointing at about the same level of precision. In particular, at the maximized openness level the performance of these algorithms is even higher than the one at the lowest openness level. However, at the begging the problem is still remaining a multi-class problem where 6 known classed have been given to the algorithms for training and 7 for testing where only 1 is unknown. However, at the highest openness level the problem is becoming a binary problem as in 1-vs-Rest. That is, the algorithms are trained at 1 class and tested at 7 classes with 6 of them as unknown. Given, that these algorithms are all variations based on SVM algorithms the results are short of expected. However, this is not always the case as it will be shown in chapter \ref{chap:noise}.


\subsection{Domain Transfer Measure}\label{chap:eval_methods:sec:domain_transfer_measure}

\textit{Domain Transfer Evaluation (DTE)} is a practical methodology for evaluating the classification performance of an text-mining algorithm. The goal of this evaluation methodology is to measure the generalization of the algorithm's induced model when the training corpus is rather small. Thus, with the domain transfer evaluation the algorithm's performance is tested in an unknown domain, for the same text-mining task. 

Particularly for the WGI, with this measure we can evaluate an algorithm that has been trained to identify \textit{News} or \textit{Wiki} genres. Then by testing it on \textit{Blog}, we could evaluate the model in such a case when very small corpus is available for training. Also, DTE can be applied for evaluating the model's behavior upon changes of the type of \textit{features} have been selected, e.g. BOW, POS, Term N-grams etc. 

The performance it can be measured using Accuracy, F1-statistic, Precision-Recall Curve, Receiver Operating Characteristic (ROC) Curve etc, and then compare the two measures pairwise for every domain combination (e.g. $\{News, Sports\}$, etc).

The measure proposed from \parencite{finn2006learning} where equation \ref{chap:eval_methods:eq:office_doc_ensemble} is its generalized form. Originally, this measure was designed for \textit{Accuracy} in mind. However, it can be used for any score such as F1. In order to fit in the open-set framework.

\begin{equation} \label{chap:eval_methods:eq:office_doc_ensemble}
	T^{C,F} = \frac{1}{N(N-1)} \sum_{A=1}^{N} \sum_{B, \forall B \neq A}^{N} \left(  \frac{M^{C,F}_{A,B}}{M^{C,F}_{A,A}} \right)
\end{equation}

\noindent	
where T is the \textit{Transfer Measure Score}, M is the measure of choice (Accuracy, F1, Precision, Recall, etc), F is the \textit{Feature Set}, and C is the \textit{Genre Class}. 

