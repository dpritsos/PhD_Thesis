\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}The Usefulness of Distributed Representation in WGI}{87}{chapter.260}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:word_embeddings}{{6}{87}{The Usefulness of Distributed Representation in WGI}{chapter.260}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{87}{section.261}}
\newlabel{chap:word_embeddings:sec:intro}{{6.1}{87}{Introduction}{section.261}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Obtaining Distributed Representations}{88}{section.262}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Word Embeddings}{88}{subsection.263}}
\newlabel{chap:word_embeddings:eq:CBOW_log_likelihood}{{6.1}{88}{Word Embeddings}{equation.265}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Diagram for C-BOW and General NLM architecture. Depending whether the $t_{i*}$ is part of the projection and the hidden layer or the layers are different. In practice the weighting matrices are either shared or diff rent between the word projection and the hidden layer or it is the same matrix, which is equivalent to the words been projected to the same position as their vectors are averaged and not concatenated.\relax }}{89}{figure.caption.264}}
\newlabel{chap:word_embeddingss:fig:CBOW_diagram}{{6.1}{89}{Diagram for C-BOW and General NLM architecture. Depending whether the $t_{i*}$ is part of the projection and the hidden layer or the layers are different. In practice the weighting matrices are either shared or diff rent between the word projection and the hidden layer or it is the same matrix, which is equivalent to the words been projected to the same position as their vectors are averaged and not concatenated.\relax }{figure.caption.264}{}}
\newlabel{chap:word_embeddings:eq:CBOW_softmax}{{6.2}{89}{Word Embeddings}{equation.266}{}}
\newlabel{chap:word_embeddings:eq:skipgram_log_likelihood}{{6.3}{89}{Word Embeddings}{equation.267}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Diagram for Skip-Gram.\relax }}{90}{figure.caption.269}}
\newlabel{chap:word_embeddingss:fig:skipgram_diagram}{{6.2}{90}{Diagram for Skip-Gram.\relax }{figure.caption.269}{}}
\newlabel{chap:word_embeddings:eq:skipgram_softmax}{{6.4}{90}{Word Embeddings}{equation.268}{}}
\newlabel{chap:word_embeddings:eq:nnet_condtraint}{{6.5}{90}{Word Embeddings}{equation.270}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Document Embeddings}{91}{subsection.272}}
\newlabel{chap:word_embeddings:sec:PVBOW}{{6.2.2}{91}{Document Embeddings}{subsection.272}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Diagram for PV-BOW\relax }}{92}{figure.caption.274}}
\newlabel{chap:word_embeddingss:fig:PVBOW_diagram}{{6.3}{92}{Diagram for PV-BOW\relax }{figure.caption.274}{}}
\newlabel{chap:word_embeddings:eq:pvbow_log_likelihood}{{6.6}{92}{Document Embeddings}{equation.275}{}}
\newlabel{chap:word_embeddings:eq:pvbow_softmax}{{6.7}{92}{Document Embeddings}{equation.276}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Experimental Setup}{93}{section.279}}
\newlabel{chap:word_embeddings:sec:experiments_setup}{{6.3}{93}{Experimental Setup}{section.279}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Experimental Results}{94}{section.280}}
\newlabel{chap:word_embeddings:sec:results}{{6.4}{94}{Experimental Results}{section.280}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}NNDR with DF or TF Features}{94}{subsection.281}}
\newlabel{chap:word_embeddings:sec:NNDR_PVBOW_vs_BOW}{{6.4.1}{94}{NNDR with DF or TF Features}{subsection.281}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Maximum performance of NNDR on TF Features of SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ration Threshold. $\lambda $ is the weigthing balance regulation parameters for the Normalized Accuracy. T.TYPE is the Terms Type. DIMs is the features model's dimensions. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro PR Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }}{95}{table.caption.282}}
\newlabel{chap:word_embeddings:tbl:NNDR_TF}{{6.1}{95}{Maximum performance of NNDR on TF Features of SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ration Threshold. $\lambda $ is the weigthing balance regulation parameters for the Normalized Accuracy. T.TYPE is the Terms Type. DIMs is the features model's dimensions. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro PR Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }{table.caption.282}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Maximum performance of NNDR on Distributional Features of SANTINIS corpus. STP, SUP, DRT, $\lambda $, T.TYPE, DIMs are the same as in table \ref  {tbl:NNDR_TF}. MTF is the Minimum Threshold Fequency of the Distributional models Vocabulary. WS is the Windows Size of the text sentence. $\alpha $ is the NNet parameter. EP is the epochs number of the NNet model. DEC is the decay parameter of the NNet model. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro PR Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }}{95}{table.caption.283}}
\newlabel{chap:word_embeddings:tbl:NNDR_PVBOW}{{6.2}{95}{Maximum performance of NNDR on Distributional Features of SANTINIS corpus. STP, SUP, DRT, $\lambda $, T.TYPE, DIMs are the same as in table \ref {tbl:NNDR_TF}. MTF is the Minimum Threshold Fequency of the Distributional models Vocabulary. WS is the Windows Size of the text sentence. $\alpha $ is the NNet parameter. EP is the epochs number of the NNet model. DEC is the decay parameter of the NNet model. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro PR Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }{table.caption.283}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Precision-Recall Curves of NNDR on SANTINIS corpus. The curves are for W3G terms-type and for AUC optimization criterion. The 11-recall-levels are shown for each evaluation experiment. The lines are stopping before the 11th recall level due to the open-set framework, i.e. the remaining part after the last mark of each curve it the percentage of the corpus tha has been classified as Unknown from the algorithms.\relax }}{96}{figure.caption.284}}
\newlabel{chap:word_embeddings:fig:NNDR_W3G}{{6.4}{96}{Precision-Recall Curves of NNDR on SANTINIS corpus. The curves are for W3G terms-type and for AUC optimization criterion. The 11-recall-levels are shown for each evaluation experiment. The lines are stopping before the 11th recall level due to the open-set framework, i.e. the remaining part after the last mark of each curve it the percentage of the corpus tha has been classified as Unknown from the algorithms.\relax }{figure.caption.284}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Comparison of Open-set WGI Methods}{96}{subsection.285}}
\newlabel{chap:word_embeddings:sec:experiments_setup}{{6.4.2}{96}{Comparison of Open-set WGI Methods}{subsection.285}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Performance of baselines and NNDR on the SANTINIS coprus. All evaluation scores are macro-averaged.\relax }}{97}{table.caption.286}}
\newlabel{chap:word_embeddings:tbl:NNDR_RFSE_OCSVME_final}{{6.3}{97}{Performance of baselines and NNDR on the SANTINIS coprus. All evaluation scores are macro-averaged.\relax }{table.caption.286}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Precision curves in 11-standard recall levels of the examined open-set classifiers using either W3G features (left) or W1G features (right).\relax }}{98}{figure.caption.287}}
\newlabel{chap:word_embeddings:fig:NNDR_W3G_Best_RFSE_Baseline}{{6.5}{98}{Precision curves in 11-standard recall levels of the examined open-set classifiers using either W3G features (left) or W1G features (right).\relax }{figure.caption.287}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Conclusions}{98}{section.288}}
\newlabel{chap:word_embeddings:sec:conclusions}{{6.5}{98}{Conclusions}{section.288}{}}
\@setckpt{Chapters/Word_embedding}{
\setcounter{page}{100}
\setcounter{equation}{7}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{4}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{6}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{3}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{float@type}{8}
\setcounter{AlgoLine}{15}
\setcounter{algocfline}{3}
\setcounter{algocfproc}{3}
\setcounter{algocf}{0}
\setcounter{definition}{4}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{324}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextradate}{3}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{34}
\setcounter{Hfootnote}{11}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{65}
\setcounter{lstlisting}{0}
\setcounter{section@level}{1}
}
