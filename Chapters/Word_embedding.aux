\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Open-set WGI with Neural Language Modeling}{83}{chapter.275}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:word_embeddings}{{6}{83}{Open-set WGI with Neural Language Modeling}{chapter.275}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{83}{section.276}}
\newlabel{chap:word_embeddings:sec:intro}{{6.1}{83}{Introduction}{section.276}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Neural Language Models}{83}{section.277}}
\newlabel{chap:word_embeddings:sec:NLM}{{6.2}{83}{Neural Language Models}{section.277}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}N-grams, Distributional Features and Word Embeddings}{83}{subsection.278}}
\newlabel{chap:word_embeddings:sec:ngrans_vs_doc2vec}{{6.2.1}{83}{N-grams, Distributional Features and Word Embeddings}{subsection.278}{}}
\newlabel{chap:word_embeddings:eq:slm}{{6.1}{84}{N-grams, Distributional Features and Word Embeddings}{equation.279}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Diagram for C-BOW and General NLM architecture. Depending whether the $t_{i*}$ is part of the projection and the hidden layer or the layers are different. In practice the weighting matrices are either shared or diff rent between the word projection and the hidden layer or it is the same matrix, which is equivalent to the words been projected to the same position as their vectors are averaged and not concatenated.\relax }}{85}{figure.caption.280}}
\newlabel{chap:word_embeddingss:fig:CBOW_diagram}{{6.1}{85}{Diagram for C-BOW and General NLM architecture. Depending whether the $t_{i*}$ is part of the projection and the hidden layer or the layers are different. In practice the weighting matrices are either shared or diff rent between the word projection and the hidden layer or it is the same matrix, which is equivalent to the words been projected to the same position as their vectors are averaged and not concatenated.\relax }{figure.caption.280}{}}
\newlabel{chap:word_embeddings:eq:NLM}{{6.2}{85}{N-grams, Distributional Features and Word Embeddings}{equation.281}{}}
\newlabel{chap:word_embeddings:eq:CBOW}{{6.3}{86}{N-grams, Distributional Features and Word Embeddings}{equation.282}{}}
\newlabel{chap:word_embeddings:eq:CBOW_softmax}{{6.4}{86}{N-grams, Distributional Features and Word Embeddings}{equation.283}{}}
\newlabel{chap:word_embeddings:eq:CBOW_log_likelihood}{{6.5}{86}{N-grams, Distributional Features and Word Embeddings}{equation.284}{}}
\newlabel{chap:word_embeddings:eq:skipgram_log_likelihood}{{6.6}{86}{N-grams, Distributional Features and Word Embeddings}{equation.285}{}}
\newlabel{chap:word_embeddings:eq:skipgram_softmax}{{6.7}{86}{N-grams, Distributional Features and Word Embeddings}{equation.286}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Diagram for Skip-Gram.\relax }}{87}{figure.caption.287}}
\newlabel{chap:word_embeddingss:fig:skipgram_diagram}{{6.2}{87}{Diagram for Skip-Gram.\relax }{figure.caption.287}{}}
\newlabel{chap:word_embeddings:eq:nnet_condtraint}{{6.8}{87}{N-grams, Distributional Features and Word Embeddings}{equation.288}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Paragraph-Vector Bag-of-Words and Document Vectors Projection}{88}{subsection.289}}
\newlabel{chap:word_embeddings:sec:PVBOW}{{6.2.2}{88}{Paragraph-Vector Bag-of-Words and Document Vectors Projection}{subsection.289}{}}
\newlabel{chap:word_embeddings:eq:pvbow_log_likelihood}{{6.9}{88}{Paragraph-Vector Bag-of-Words and Document Vectors Projection}{equation.291}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Diagram for PV-BOW\relax }}{89}{figure.caption.290}}
\newlabel{chap:word_embeddingss:fig:PVBOW_diagram}{{6.3}{89}{Diagram for PV-BOW\relax }{figure.caption.290}{}}
\newlabel{chap:word_embeddings:eq:pvbow_softmax}{{6.10}{89}{Paragraph-Vector Bag-of-Words and Document Vectors Projection}{equation.292}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Experiments}{90}{section.295}}
\newlabel{chap:word_embeddings:sec:experiments_setup}{{6.3}{90}{Experiments}{section.295}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Corpus}{90}{subsection.296}}
\newlabel{chap:word_embeddings:sec:experiments_corpora}{{6.3.1}{90}{Corpus}{subsection.296}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Open-set Models Parameters Setup}{90}{subsection.297}}
\newlabel{chap:word_embeddings:sec:experiments_params}{{6.3.2}{90}{Open-set Models Parameters Setup}{subsection.297}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Results}{91}{section.298}}
\newlabel{chap:word_embeddings:sec:results}{{6.4}{91}{Results}{section.298}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Paragraph-Vectors vs. N-Gram-Vectors(TF): Improving Low Performance Learners}{91}{subsection.299}}
\newlabel{chap:word_embeddings:sec:NNDR_PVBOW_vs_BOW}{{6.4.1}{91}{Paragraph-Vectors vs. N-Gram-Vectors(TF): Improving Low Performance Learners}{subsection.299}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Maximum performance of NNDR on TF Features of SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ration Threshold. $\lambda $ is the weigthing balance regulation parameters for the Normalized Accuracy. T.TYPE is the Terms Type. DIMs is the features model's dimensions. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro PR Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }}{92}{table.caption.300}}
\newlabel{chap:word_embeddings:tbl:NNDR_TF}{{6.1}{92}{Maximum performance of NNDR on TF Features of SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ration Threshold. $\lambda $ is the weigthing balance regulation parameters for the Normalized Accuracy. T.TYPE is the Terms Type. DIMs is the features model's dimensions. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro PR Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }{table.caption.300}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Maximum performance of NNDR on Distributional Features of SANTINIS corpus. STP, SUP, DRT, $\lambda $, T.TYPE, DIMs are the same as in table \ref  {tbl:NNDR_TF}. MTF is the Minimum Threshold Fequency of the Distributional models Vocabulary. WS is the Windows Size of the text sentence. $\alpha $ is the NNet parameter. EP is the epochs number of the NNet model. DEC is the decay parameter of the NNet model. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro PR Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }}{92}{table.caption.301}}
\newlabel{chap:word_embeddings:tbl:NNDR_PVBOW}{{6.2}{92}{Maximum performance of NNDR on Distributional Features of SANTINIS corpus. STP, SUP, DRT, $\lambda $, T.TYPE, DIMs are the same as in table \ref {tbl:NNDR_TF}. MTF is the Minimum Threshold Fequency of the Distributional models Vocabulary. WS is the Windows Size of the text sentence. $\alpha $ is the NNet parameter. EP is the epochs number of the NNet model. DEC is the decay parameter of the NNet model. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro PR Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }{table.caption.301}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Precision-Recall Curves of NNDR on SANTINIS corpus. The curves are for W3G terms-type and for AUC optimization criterion. The 11-recall-levels are shown for each evaluation experiment. The lines are stopping before the 11th recall level due to the open-set framework, i.e. the remaining part after the last mark of each curve it the percentage of the corpus tha has been classified as Unknown from the algorithms.\relax }}{93}{figure.caption.302}}
\newlabel{chap:word_embeddings:fig:NNDR_W3G}{{6.4}{93}{Precision-Recall Curves of NNDR on SANTINIS corpus. The curves are for W3G terms-type and for AUC optimization criterion. The 11-recall-levels are shown for each evaluation experiment. The lines are stopping before the 11th recall level due to the open-set framework, i.e. the remaining part after the last mark of each curve it the percentage of the corpus tha has been classified as Unknown from the algorithms.\relax }{figure.caption.302}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Nearest Neighbours Distance Ration with Paragraph-Vectors}{93}{subsection.303}}
\newlabel{chap:word_embeddings:sec:experiments_setup}{{6.4.2}{93}{Nearest Neighbours Distance Ration with Paragraph-Vectors}{subsection.303}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Performance of baselines and NNDR on the SANTINIS coprus. All evaluation scores are macro-averaged.\relax }}{94}{table.caption.304}}
\newlabel{chap:word_embeddings:tbl:NNDR_RFSE_OCSVME_final}{{6.3}{94}{Performance of baselines and NNDR on the SANTINIS coprus. All evaluation scores are macro-averaged.\relax }{table.caption.304}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Precision curves in 11-standard recall levels of the examined open-set classifiers using either W3G features (left) or W1G features (right).\relax }}{95}{figure.caption.305}}
\newlabel{chap:word_embeddings:fig:NNDR_W3G_Best_RFSE_Baseline}{{6.5}{95}{Precision curves in 11-standard recall levels of the examined open-set classifiers using either W3G features (left) or W1G features (right).\relax }{figure.caption.305}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Conclusions}{95}{section.306}}
\newlabel{chap:word_embeddings:sec:conclusions}{{6.5}{95}{Conclusions}{section.306}{}}
\@setckpt{Chapters/Word_embedding}{
\setcounter{page}{97}
\setcounter{equation}{10}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{6}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{3}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{float@type}{8}
\setcounter{AlgoLine}{14}
\setcounter{algocfline}{4}
\setcounter{algocfproc}{4}
\setcounter{algocf}{0}
\setcounter{definition}{6}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{325}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextradate}{3}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{34}
\setcounter{Hfootnote}{9}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{67}
\setcounter{lstlisting}{0}
\setcounter{section@level}{1}
}
