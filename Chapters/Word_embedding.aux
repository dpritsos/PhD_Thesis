\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}The Usefulness of Distributed Representations in WGI}{89}{chapter.222}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:word_embeddings}{{6}{89}{The Usefulness of Distributed Representations in WGI}{chapter.222}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{89}{section.223}}
\newlabel{chap:word_embeddings:sec:intro}{{6.1}{89}{Introduction}{section.223}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Obtaining Distributed Representations}{90}{section.224}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Word Embeddings}{90}{subsection.225}}
\newlabel{chap:word_embeddings:eq:CBOW_log_likelihood}{{6.1}{91}{Word Embeddings}{equation.227}{}}
\newlabel{chap:word_embeddings:eq:CBOW_softmax}{{6.2}{91}{Word Embeddings}{equation.228}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Architecture of the C-BOW model. The network attempts to predict a word given its context words. The order of input words is ignored. The hidden layer has much lower dimensionality in comparison to the one-hot representation of input and output words. The learned weights in $W_{in}$ (and $W_{out}$) can be used as word embeddings.\relax }}{92}{figure.caption.226}}
\newlabel{chap:word_embeddingss:fig:CBOW_diagram}{{6.1}{92}{Architecture of the C-BOW model. The network attempts to predict a word given its context words. The order of input words is ignored. The hidden layer has much lower dimensionality in comparison to the one-hot representation of input and output words. The learned weights in $W_{in}$ (and $W_{out}$) can be used as word embeddings.\relax }{figure.caption.226}{}}
\newlabel{chap:word_embeddings:eq:skipgram_log_likelihood}{{6.3}{92}{Word Embeddings}{equation.229}{}}
\newlabel{chap:word_embeddings:eq:skipgram_softmax}{{6.4}{92}{Word Embeddings}{equation.230}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Architecture of skip-gram model. Given a word the network tries to predict its context words. The dimensionality of the hidden layer is much lower than the one-hot representation of input and output words. The learned weights in $W_{in}$ (and $W_{out}$) can be used as word embeddings.\relax }}{93}{figure.caption.231}}
\newlabel{chap:word_embeddingss:fig:skipgram_diagram}{{6.2}{93}{Architecture of skip-gram model. Given a word the network tries to predict its context words. The dimensionality of the hidden layer is much lower than the one-hot representation of input and output words. The learned weights in $W_{in}$ (and $W_{out}$) can be used as word embeddings.\relax }{figure.caption.231}{}}
\newlabel{chap:word_embeddings:eq:nnet_condtraint}{{6.5}{93}{Word Embeddings}{equation.232}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Document Embeddings}{94}{subsection.234}}
\newlabel{chap:word_embeddings:sec:PVBOW}{{6.2.2}{94}{Document Embeddings}{subsection.234}{}}
\newlabel{chap:word_embeddings:eq:pvbow_log_likelihood}{{6.6}{95}{Document Embeddings}{equation.236}{}}
\newlabel{chap:word_embeddings:eq:pvbow_softmax}{{6.7}{95}{Document Embeddings}{equation.237}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Diagram for PV-BOW\relax }}{96}{figure.caption.238}}
\newlabel{chap:word_embeddingss:fig:PVBOW_diagram}{{6.3}{96}{Diagram for PV-BOW\relax }{figure.caption.238}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Experimental Setup}{96}{section.239}}
\newlabel{chap:word_embeddings:sec:experiments_setup}{{6.3}{96}{Experimental Setup}{section.239}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Experimental Results}{98}{section.242}}
\newlabel{chap:word_embeddings:sec:results}{{6.4}{98}{Experimental Results}{section.242}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}The Effect of Distributed Representation on NNDR}{98}{subsection.243}}
\newlabel{chap:word_embeddings:sec:NNDR_PVBOW_vs_BOW}{{6.4.1}{98}{The Effect of Distributed Representation on NNDR}{subsection.243}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Maximum performance of NNDR with traditional (TF) Features on SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ratio Threshold. $\lambda $ is the regulation parameter used in the Normalized Accuracy. Dim. is the dimensionality of representation. The evaluation measures are the open-set variants of macro-averaged precision, recall, $F_1$, and AUC of the precision-recall curve.\relax }}{99}{table.caption.244}}
\newlabel{chap:word_embeddings:tbl:NNDR_TF}{{6.1}{99}{Maximum performance of NNDR with traditional (TF) Features on SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ratio Threshold. $\lambda $ is the regulation parameter used in the Normalized Accuracy. Dim. is the dimensionality of representation. The evaluation measures are the open-set variants of macro-averaged precision, recall, $F_1$, and AUC of the precision-recall curve.\relax }{table.caption.244}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Maximum performance of NNDR with distributed features on SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ratio Threshold. $\lambda $ is the regulation parameter used in the Normalized Accuracy. Dim. is the dimensionality of representation. The evaluation measures are the open-set variants of macro-averaged precision, recall, $F_1$, and AUC of the precision-recall curve.\relax }}{99}{table.caption.245}}
\newlabel{chap:word_embeddings:tbl:NNDR_PVBOW}{{6.2}{99}{Maximum performance of NNDR with distributed features on SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ratio Threshold. $\lambda $ is the regulation parameter used in the Normalized Accuracy. Dim. is the dimensionality of representation. The evaluation measures are the open-set variants of macro-averaged precision, recall, $F_1$, and AUC of the precision-recall curve.\relax }{table.caption.245}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Comparison of Open-set WGI Methods}{99}{subsection.247}}
\newlabel{chap:word_embeddings:sec:experiments_setup}{{6.4.2}{99}{Comparison of Open-set WGI Methods}{subsection.247}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Precision-Recall curves of NNDR on SANTINIS corpus for traditional (TF) and distributed (DF) W3G features.\relax }}{100}{figure.caption.246}}
\newlabel{chap:word_embeddings:fig:NNDR_W3G}{{6.4}{100}{Precision-Recall curves of NNDR on SANTINIS corpus for traditional (TF) and distributed (DF) W3G features.\relax }{figure.caption.246}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Performance of baselines and NNDR on the SANTINIS coprus. All evaluation scores are macro-averaged.\relax }}{101}{table.caption.248}}
\newlabel{chap:word_embeddings:tbl:NNDR_RFSE_OCSVME_final}{{6.3}{101}{Performance of baselines and NNDR on the SANTINIS coprus. All evaluation scores are macro-averaged.\relax }{table.caption.248}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Precision curves in 11-standard recall levels of the examined open-set classifiers using either W3G features (left) or W1G features (right).\relax }}{102}{figure.caption.249}}
\newlabel{chap:word_embeddings:fig:NNDR_W3G_Best_RFSE_Baseline}{{6.5}{102}{Precision curves in 11-standard recall levels of the examined open-set classifiers using either W3G features (left) or W1G features (right).\relax }{figure.caption.249}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Conclusions}{102}{section.250}}
\newlabel{chap:word_embeddings:sec:conclusions}{{6.5}{102}{Conclusions}{section.250}{}}
\@setckpt{Chapters/Word_embedding}{
\setcounter{page}{103}
\setcounter{equation}{7}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{4}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{6}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{3}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{float@type}{8}
\setcounter{AlgoLine}{15}
\setcounter{algocfline}{3}
\setcounter{algocfproc}{3}
\setcounter{algocf}{0}
\setcounter{definition}{4}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{236}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextradate}{3}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{22}
\setcounter{Hfootnote}{10}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{66}
\setcounter{lstlisting}{0}
\setcounter{section@level}{1}
}
