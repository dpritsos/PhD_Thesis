\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}The Usefulness of Distributed Representations in WGI}{87}{chapter.227}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:word_embeddings}{{6}{87}{The Usefulness of Distributed Representations in WGI}{chapter.227}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{87}{section.228}}
\newlabel{chap:word_embeddings:sec:intro}{{6.1}{87}{Introduction}{section.228}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Obtaining Distributed Representations}{88}{section.229}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Word Embeddings}{88}{subsection.230}}
\newlabel{chap:word_embeddings:eq:CBOW_log_likelihood}{{6.1}{89}{Word Embeddings}{equation.232}{}}
\newlabel{chap:word_embeddings:eq:CBOW_softmax}{{6.2}{89}{Word Embeddings}{equation.233}{}}
\newlabel{chap:word_embeddings:eq:skipgram_log_likelihood}{{6.3}{89}{Word Embeddings}{equation.234}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Architecture of the C-BOW model \parencite {mitra2018introduction}. The network attempts to predict a word given its context words. The order of input words is ignored. The hidden layer has much lower dimensionality in comparison to the one-hot representation of input and output words. The learned weights in $W_{in}$ (and $W_{out}$) can be used as word embeddings.\relax }}{90}{figure.caption.231}}
\newlabel{chap:word_embeddingss:fig:CBOW_diagram}{{6.1}{90}{Architecture of the C-BOW model \parencite {mitra2018introduction}. The network attempts to predict a word given its context words. The order of input words is ignored. The hidden layer has much lower dimensionality in comparison to the one-hot representation of input and output words. The learned weights in $W_{in}$ (and $W_{out}$) can be used as word embeddings.\relax }{figure.caption.231}{}}
\newlabel{chap:word_embeddings:eq:skipgram_softmax}{{6.4}{90}{Word Embeddings}{equation.235}{}}
\newlabel{chap:word_embeddings:eq:nnet_condtraint}{{6.5}{90}{Word Embeddings}{equation.237}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Architecture of skip-gram model \parencite {mitra2018introduction}. Given a word the network tries to predict its context words. The dimensionality of the hidden layer is much lower than the one-hot representation of input and output words. The learned weights in $W_{in}$ (and $W_{out}$) can be used as word embeddings.\relax }}{91}{figure.caption.236}}
\newlabel{chap:word_embeddingss:fig:skipgram_diagram}{{6.2}{91}{Architecture of skip-gram model \parencite {mitra2018introduction}. Given a word the network tries to predict its context words. The dimensionality of the hidden layer is much lower than the one-hot representation of input and output words. The learned weights in $W_{in}$ (and $W_{out}$) can be used as word embeddings.\relax }{figure.caption.236}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Document Embeddings}{92}{subsection.239}}
\newlabel{chap:word_embeddings:sec:PVBOW}{{6.2.2}{92}{Document Embeddings}{subsection.239}{}}
\newlabel{chap:word_embeddings:eq:pvbow_log_likelihood}{{6.6}{93}{Document Embeddings}{equation.241}{}}
\newlabel{chap:word_embeddings:eq:pvbow_softmax}{{6.7}{93}{Document Embeddings}{equation.242}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Architecture of PV-DBOW. Given a paragraph vector, predict context words.\relax }}{94}{figure.caption.243}}
\newlabel{chap:word_embeddingss:fig:PVBOW_diagram}{{6.3}{94}{Architecture of PV-DBOW. Given a paragraph vector, predict context words.\relax }{figure.caption.243}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Experimental Setup}{95}{section.244}}
\newlabel{chap:word_embeddings:sec:experiments_setup}{{6.3}{95}{Experimental Setup}{section.244}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Experimental Results}{96}{section.247}}
\newlabel{chap:word_embeddings:sec:results}{{6.4}{96}{Experimental Results}{section.247}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}The Effect of Distributed Representation on NNDR}{96}{subsection.248}}
\newlabel{chap:word_embeddings:sec:NNDR_PVBOW_vs_BOW}{{6.4.1}{96}{The Effect of Distributed Representation on NNDR}{subsection.248}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Maximum performance of NNDR with traditional (TF) Features on SANTINIS coprus. $p_{1}$ and $p_{2}$ are the splitting ratios to form simulated noise and DRT is the threshold. $\lambda $ is the regulation parameter used in the normalized accuracy. Dim. is the dimensionality of representation. The evaluation measures are the open-set variants of macro-averaged precision, recall, $F_1$, and AUC of the precision-recall curve.\relax }}{97}{table.caption.249}}
\newlabel{chap:word_embeddings:tbl:NNDR_TF}{{6.1}{97}{Maximum performance of NNDR with traditional (TF) Features on SANTINIS coprus. $p_{1}$ and $p_{2}$ are the splitting ratios to form simulated noise and DRT is the threshold. $\lambda $ is the regulation parameter used in the normalized accuracy. Dim. is the dimensionality of representation. The evaluation measures are the open-set variants of macro-averaged precision, recall, $F_1$, and AUC of the precision-recall curve.\relax }{table.caption.249}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Maximum performance of NNDR with distributed features on SANTINIS coprus. $p_{1}$ and $p_{2}$ are the splitting ratios to form simulated noise and DRT is the threshold. $\lambda $ is the regulation parameter used in the normalized accuracy. Dim. is the dimensionality of representation. The evaluation measures are the open-set variants of macro-averaged precision, recall, $F_1$, and AUC of the precision-recall curve.\relax }}{97}{table.caption.250}}
\newlabel{chap:word_embeddings:tbl:NNDR_PVBOW}{{6.2}{97}{Maximum performance of NNDR with distributed features on SANTINIS coprus. $p_{1}$ and $p_{2}$ are the splitting ratios to form simulated noise and DRT is the threshold. $\lambda $ is the regulation parameter used in the normalized accuracy. Dim. is the dimensionality of representation. The evaluation measures are the open-set variants of macro-averaged precision, recall, $F_1$, and AUC of the precision-recall curve.\relax }{table.caption.250}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Comparison of Open-set WGI Methods}{97}{subsection.252}}
\newlabel{chap:word_embeddings:sec:experiments_setup}{{6.4.2}{97}{Comparison of Open-set WGI Methods}{subsection.252}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Precision-Recall curves of NNDR on SANTINIS corpus for traditional (TF) and distributed (DF) W3G features.\relax }}{98}{figure.caption.251}}
\newlabel{chap:word_embeddings:fig:NNDR_W3G}{{6.4}{98}{Precision-Recall curves of NNDR on SANTINIS corpus for traditional (TF) and distributed (DF) W3G features.\relax }{figure.caption.251}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Performance of baselines and NNDR on the SANTINIS coprus. All evaluation scores are macro-averaged.\relax }}{99}{table.caption.253}}
\newlabel{chap:word_embeddings:tbl:NNDR_RFSE_OCSVME_final}{{6.3}{99}{Performance of baselines and NNDR on the SANTINIS coprus. All evaluation scores are macro-averaged.\relax }{table.caption.253}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Precision curves in 11-standard recall levels of the examined open-set classifiers using either W3G features (left) or W1G features (right).\relax }}{100}{figure.caption.254}}
\newlabel{chap:word_embeddings:fig:NNDR_W3G_Best_RFSE_Baseline}{{6.5}{100}{Precision curves in 11-standard recall levels of the examined open-set classifiers using either W3G features (left) or W1G features (right).\relax }{figure.caption.254}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Conclusions}{100}{section.255}}
\newlabel{chap:word_embeddings:sec:conclusions}{{6.5}{100}{Conclusions}{section.255}{}}
\@setckpt{Chapters/Word_embedding}{
\setcounter{page}{101}
\setcounter{equation}{7}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{4}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{6}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{3}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{float@type}{8}
\setcounter{AlgoLine}{15}
\setcounter{algocfline}{3}
\setcounter{algocfproc}{3}
\setcounter{algocf}{0}
\setcounter{definition}{4}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{319}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextradate}{3}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{22}
\setcounter{Hfootnote}{10}
\setcounter{bookmark@seq@number}{64}
\setcounter{lstlisting}{0}
\setcounter{section@level}{1}
}
