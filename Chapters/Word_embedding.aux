\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}The Usefulness of Distributed Representations in WGI}{103}{chapter.256}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:word_embeddings}{{6}{103}{The Usefulness of Distributed Representations in WGI}{chapter.256}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{103}{section.257}}
\newlabel{chap:word_embeddings:sec:intro}{{6.1}{103}{Introduction}{section.257}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Obtaining Distributed Representations}{104}{section.258}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Word Embeddings}{104}{subsection.259}}
\newlabel{chap:word_embeddings:eq:CBOW_log_likelihood}{{6.1}{105}{Word Embeddings}{equation.261}{}}
\newlabel{chap:word_embeddings:eq:CBOW_softmax}{{6.2}{105}{Word Embeddings}{equation.262}{}}
\newlabel{chap:word_embeddings:eq:skipgram_log_likelihood}{{6.3}{105}{Word Embeddings}{equation.263}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Architecture of the C-BOW model. The network attempts to predict a word given its context words. The order of input words is ignored. The hidden layer has much lower dimensionality in comparison to the one-hot representation of input and output words. The learned weights in $W_{in}$ (and $W_{out}$) can be used as word embeddings.\relax }}{106}{figure.caption.260}}
\newlabel{chap:word_embeddingss:fig:CBOW_diagram}{{6.1}{106}{Architecture of the C-BOW model. The network attempts to predict a word given its context words. The order of input words is ignored. The hidden layer has much lower dimensionality in comparison to the one-hot representation of input and output words. The learned weights in $W_{in}$ (and $W_{out}$) can be used as word embeddings.\relax }{figure.caption.260}{}}
\newlabel{chap:word_embeddings:eq:skipgram_softmax}{{6.4}{106}{Word Embeddings}{equation.264}{}}
\newlabel{chap:word_embeddings:eq:nnet_condtraint}{{6.5}{106}{Word Embeddings}{equation.266}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Architecture of skip-gram model. Given a word the network tries to predict its context words. The dimensionality of the hidden layer is much lower than the one-hot representation of input and output words. The learned weights in $W_{in}$ (and $W_{out}$) can be used as word embeddings.\relax }}{107}{figure.caption.265}}
\newlabel{chap:word_embeddingss:fig:skipgram_diagram}{{6.2}{107}{Architecture of skip-gram model. Given a word the network tries to predict its context words. The dimensionality of the hidden layer is much lower than the one-hot representation of input and output words. The learned weights in $W_{in}$ (and $W_{out}$) can be used as word embeddings.\relax }{figure.caption.265}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Document Embeddings}{108}{subsection.268}}
\newlabel{chap:word_embeddings:sec:PVBOW}{{6.2.2}{108}{Document Embeddings}{subsection.268}{}}
\newlabel{chap:word_embeddings:eq:pvbow_log_likelihood}{{6.6}{109}{Document Embeddings}{equation.270}{}}
\newlabel{chap:word_embeddings:eq:pvbow_softmax}{{6.7}{109}{Document Embeddings}{equation.271}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Diagram for PV-BOW\relax }}{110}{figure.caption.272}}
\newlabel{chap:word_embeddingss:fig:PVBOW_diagram}{{6.3}{110}{Diagram for PV-BOW\relax }{figure.caption.272}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Experimental Setup}{110}{section.273}}
\newlabel{chap:word_embeddings:sec:experiments_setup}{{6.3}{110}{Experimental Setup}{section.273}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Experimental Results}{112}{section.276}}
\newlabel{chap:word_embeddings:sec:results}{{6.4}{112}{Experimental Results}{section.276}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}The Effect of Distributed Representation on NNDR}{112}{subsection.277}}
\newlabel{chap:word_embeddings:sec:NNDR_PVBOW_vs_BOW}{{6.4.1}{112}{The Effect of Distributed Representation on NNDR}{subsection.277}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Maximum performance of NNDR with traditional (TF) Features on SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ratio Threshold. $\lambda $ is the regulation parameter used in the Normalized Accuracy. Dim. is the dimensionality of representation. The evaluation measures are the open-set variants of macro-averaged precision, recall, $F_1$, and AUC of the precision-recall curve.\relax }}{113}{table.caption.278}}
\newlabel{chap:word_embeddings:tbl:NNDR_TF}{{6.1}{113}{Maximum performance of NNDR with traditional (TF) Features on SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ratio Threshold. $\lambda $ is the regulation parameter used in the Normalized Accuracy. Dim. is the dimensionality of representation. The evaluation measures are the open-set variants of macro-averaged precision, recall, $F_1$, and AUC of the precision-recall curve.\relax }{table.caption.278}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Maximum performance of NNDR with distributed features on SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ratio Threshold. $\lambda $ is the regulation parameter used in the Normalized Accuracy. Dim. is the dimensionality of representation. The evaluation measures are the open-set variants of macro-averaged precision, recall, $F_1$, and AUC of the precision-recall curve.\relax }}{113}{table.caption.279}}
\newlabel{chap:word_embeddings:tbl:NNDR_PVBOW}{{6.2}{113}{Maximum performance of NNDR with distributed features on SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ratio Threshold. $\lambda $ is the regulation parameter used in the Normalized Accuracy. Dim. is the dimensionality of representation. The evaluation measures are the open-set variants of macro-averaged precision, recall, $F_1$, and AUC of the precision-recall curve.\relax }{table.caption.279}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Comparison of Open-set WGI Methods}{113}{subsection.281}}
\newlabel{chap:word_embeddings:sec:experiments_setup}{{6.4.2}{113}{Comparison of Open-set WGI Methods}{subsection.281}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Precision-Recall curves of NNDR on SANTINIS corpus for traditional (TF) and distributed (DF) W3G features.\relax }}{114}{figure.caption.280}}
\newlabel{chap:word_embeddings:fig:NNDR_W3G}{{6.4}{114}{Precision-Recall curves of NNDR on SANTINIS corpus for traditional (TF) and distributed (DF) W3G features.\relax }{figure.caption.280}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Performance of baselines and NNDR on the SANTINIS coprus. All evaluation scores are macro-averaged.\relax }}{115}{table.caption.282}}
\newlabel{chap:word_embeddings:tbl:NNDR_RFSE_OCSVME_final}{{6.3}{115}{Performance of baselines and NNDR on the SANTINIS coprus. All evaluation scores are macro-averaged.\relax }{table.caption.282}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Precision curves in 11-standard recall levels of the examined open-set classifiers using either W3G features (left) or W1G features (right).\relax }}{115}{figure.caption.283}}
\newlabel{chap:word_embeddings:fig:NNDR_W3G_Best_RFSE_Baseline}{{6.5}{115}{Precision curves in 11-standard recall levels of the examined open-set classifiers using either W3G features (left) or W1G features (right).\relax }{figure.caption.283}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Conclusions}{116}{section.284}}
\newlabel{chap:word_embeddings:sec:conclusions}{{6.5}{116}{Conclusions}{section.284}{}}
\@setckpt{Chapters/Word_embedding}{
\setcounter{page}{117}
\setcounter{equation}{7}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{4}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{6}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{3}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{float@type}{8}
\setcounter{AlgoLine}{15}
\setcounter{algocfline}{3}
\setcounter{algocfproc}{3}
\setcounter{algocf}{0}
\setcounter{definition}{4}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{308}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextradate}{3}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{34}
\setcounter{Hfootnote}{11}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{74}
\setcounter{lstlisting}{0}
\setcounter{section@level}{1}
}
