\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Open-set WGI with Distributional Features}{73}{chapter.269}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:word_embeddings}{{6}{73}{Open-set WGI with Distributional Features}{chapter.269}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{73}{section.270}}
\newlabel{chap:word_embeddings:sec:intro}{{6.1}{73}{Introduction}{section.270}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}N-grams, Word Embeddings and Distributional Features}{73}{section.271}}
\newlabel{chap:word_embeddings:sec:ngrans_vs_doc2vec}{{6.2}{73}{N-grams, Word Embeddings and Distributional Features}{section.271}{}}
\newlabel{chap:word_embeddings:eq:slm}{{6.1}{74}{N-grams, Word Embeddings and Distributional Features}{equation.272}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Diagram for C-BOW and General NLM architecture. Depending whether the $t_{i*}$ is part of the projection and the hidden layer or the layers are different. In practice the weighting matrices are either shared or diff rent between the word projection and the hidden layer or it is the same matrix, which is equivalent to the words been projected to the same position as their vectors are averaged and not concatenated.\relax }}{75}{figure.caption.273}}
\newlabel{chap:word_embeddingss:fig:CBOW_diagram}{{6.1}{75}{Diagram for C-BOW and General NLM architecture. Depending whether the $t_{i*}$ is part of the projection and the hidden layer or the layers are different. In practice the weighting matrices are either shared or diff rent between the word projection and the hidden layer or it is the same matrix, which is equivalent to the words been projected to the same position as their vectors are averaged and not concatenated.\relax }{figure.caption.273}{}}
\newlabel{chap:word_embeddings:eq:NLM}{{6.2}{75}{N-grams, Word Embeddings and Distributional Features}{equation.274}{}}
\newlabel{chap:word_embeddings:eq:CBOW}{{6.3}{76}{N-grams, Word Embeddings and Distributional Features}{equation.275}{}}
\newlabel{chap:word_embeddings:eq:CBOW_softmax}{{6.4}{76}{N-grams, Word Embeddings and Distributional Features}{equation.276}{}}
\newlabel{chap:word_embeddings:eq:CBOW_log_likelihood}{{6.5}{76}{N-grams, Word Embeddings and Distributional Features}{equation.277}{}}
\newlabel{chap:word_embeddings:eq:skipgram_log_likelihood}{{6.6}{76}{N-grams, Word Embeddings and Distributional Features}{equation.278}{}}
\newlabel{chap:word_embeddings:eq:skipgram_softmax}{{6.7}{76}{N-grams, Word Embeddings and Distributional Features}{equation.279}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Diagram for Skip-Gram.\relax }}{77}{figure.caption.280}}
\newlabel{chap:word_embeddingss:fig:skipgram_diagram}{{6.2}{77}{Diagram for Skip-Gram.\relax }{figure.caption.280}{}}
\newlabel{chap:word_embeddings:eq:nnet_condtraint}{{6.8}{77}{N-grams, Word Embeddings and Distributional Features}{equation.281}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Paragraph-Vector Bag-of-Words and Document Vectors Projection}{78}{section.282}}
\newlabel{chap:word_embeddings:eq:pvbow_log_likelihood}{{6.9}{78}{Paragraph-Vector Bag-of-Words and Document Vectors Projection}{equation.284}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Diagram for PV-BOW\relax }}{79}{figure.caption.283}}
\newlabel{chap:word_embeddingss:fig:PVBOW_diagram}{{6.3}{79}{Diagram for PV-BOW\relax }{figure.caption.283}{}}
\newlabel{chap:word_embeddings:eq:pvbow_softmax}{{6.10}{79}{Paragraph-Vector Bag-of-Words and Document Vectors Projection}{equation.285}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Paragraph-Vectors for Improving Low Performance Learners on WGI}{80}{section.288}}
\newlabel{chap:word_embeddings:sec:all_algorithms_test_with_doc2vec}{{6.4}{80}{Paragraph-Vectors for Improving Low Performance Learners on WGI}{section.288}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Random Features vs Paragraph Vector Bag-of-Words for the Open-set Framework}{80}{section.289}}
\newlabel{chap:word_embeddings:sec:last_paper_presentation}{{6.5}{80}{Random Features vs Paragraph Vector Bag-of-Words for the Open-set Framework}{section.289}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Experiments}{80}{section.290}}
\newlabel{sec:experiments}{{6.6}{80}{Experiments}{section.290}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Corpus}{80}{subsection.291}}
\newlabel{sec:corpora}{{6.6.1}{80}{Corpus}{subsection.291}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Experimental Setup}{80}{subsection.292}}
\newlabel{sec:evaluation_measures}{{6.6.2}{80}{Experimental Setup}{subsection.292}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Results}{81}{section.293}}
\newlabel{sec:Experiments_Results}{{6.7}{81}{Results}{section.293}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Performance of baselines and NNDR on the SANTINIS coprus. All evaluation scores are macro-averaged.\relax }}{82}{table.caption.294}}
\newlabel{tbl:results}{{6.1}{82}{Performance of baselines and NNDR on the SANTINIS coprus. All evaluation scores are macro-averaged.\relax }{table.caption.294}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Precision curves in 11-standard recall levels of the examined open-set classifiers using either W3G features (left) or W1G features (right).\relax }}{82}{figure.caption.295}}
\newlabel{fig:NNDR_W3G_Best_RFSE_Baseline}{{6.4}{82}{Precision curves in 11-standard recall levels of the examined open-set classifiers using either W3G features (left) or W1G features (right).\relax }{figure.caption.295}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.8}Experiments}{82}{section.296}}
\newlabel{chap:word_embeddings:sec:experimental_setup}{{6.8}{82}{Experiments}{section.296}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8.1}Open-set Evaluation Methodology}{82}{subsection.297}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8.2}Corpora}{83}{subsection.299}}
\newlabel{chap:word_embeddings:sec:corpora}{{6.8.2}{83}{Corpora}{subsection.299}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8.3}Settings}{83}{subsection.300}}
\newlabel{chap:word_embeddings:sec:evaluation_measures}{{6.8.3}{83}{Settings}{subsection.300}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces SANTINIS corpora descriptions and amount of pages per genre.\relax }}{84}{table.caption.301}}
\newlabel{tbl:genre_tags}{{6.2}{84}{SANTINIS corpora descriptions and amount of pages per genre.\relax }{table.caption.301}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.9}Results}{84}{section.302}}
\newlabel{chap:word_embeddings:sec:Experiments_Results}{{6.9}{84}{Results}{section.302}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Maximum performance of NNDR on TF Features of SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ration Threshold. $\lambda $ is the weigthing balance regulation parameters for the Normalized Accuracy. T.TYPE is the Terms Type. DIMs is the features model's dimensions. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro PR Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }}{86}{table.caption.303}}
\newlabel{tbl:NNDR_TF}{{6.3}{86}{Maximum performance of NNDR on TF Features of SANTINIS coprus. STP is the Spliting Training Percentage. SUP is the Splitting Unknown Percentage. DRT is the Distance Ration Threshold. $\lambda $ is the weigthing balance regulation parameters for the Normalized Accuracy. T.TYPE is the Terms Type. DIMs is the features model's dimensions. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro PR Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }{table.caption.303}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Maximum performance of RFSE on TF Features of SANTINIS coprus. FSS is the Features Subset Selection number. $\sigma $T is the sigma threshold of the RFSE. ITER is the number of iterations of the RFSE. T.TYPE is the Terms Type. DIMs is the features model's dimensions. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro Precision-Recall Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }}{86}{table.caption.304}}
\newlabel{tbl:RFSE_TF}{{6.4}{86}{Maximum performance of RFSE on TF Features of SANTINIS coprus. FSS is the Features Subset Selection number. $\sigma $T is the sigma threshold of the RFSE. ITER is the number of iterations of the RFSE. T.TYPE is the Terms Type. DIMs is the features model's dimensions. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro Precision-Recall Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }{table.caption.304}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Maximum performance of NNDR on Distributional Features of SANTINIS corpus. STP, SUP, DRT, $\lambda $, T.TYPE, DIMs are the same as in table \ref  {tbl:NNDR_TF}. MTF is the Minimum Threshold Fequency of the Distributional models Vocabulary. WS is the Windows Size of the text sentence. $\alpha $ is the NNet parameter. EP is the epochs number of the NNet model. DEC is the decay parameter of the NNet model. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro PR Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }}{87}{table.caption.305}}
\newlabel{tbl:NNDR_Gensim}{{6.5}{87}{Maximum performance of NNDR on Distributional Features of SANTINIS corpus. STP, SUP, DRT, $\lambda $, T.TYPE, DIMs are the same as in table \ref {tbl:NNDR_TF}. MTF is the Minimum Threshold Fequency of the Distributional models Vocabulary. WS is the Windows Size of the text sentence. $\alpha $ is the NNet parameter. EP is the epochs number of the NNet model. DEC is the decay parameter of the NNet model. MP is the Macro Precision. MR is the Macro Recall. MAUC is the Area Under the Macro PR Curve. MF1 is the F1 score of the Macro Precision and Macro Recall.\relax }{table.caption.305}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Precision-Recall Curves of NNDR on SANTINIS corpus with RFSE as Baseline. The curves are for W3G terms-type and for AUC optimization criterion. The 11-recall-levels are shown for each evaluation experiment. The lines are stopping before the 11th recall level due to the open-set framework, i.e. the remaining part after the last mark of each curve it the percentage of the corpus tha has been classified as Unknown from the algorithms.\relax }}{87}{figure.caption.306}}
\newlabel{fig:NNDR_W3G_Best_RFSE_Baseline}{{6.5}{87}{Precision-Recall Curves of NNDR on SANTINIS corpus with RFSE as Baseline. The curves are for W3G terms-type and for AUC optimization criterion. The 11-recall-levels are shown for each evaluation experiment. The lines are stopping before the 11th recall level due to the open-set framework, i.e. the remaining part after the last mark of each curve it the percentage of the corpus tha has been classified as Unknown from the algorithms.\relax }{figure.caption.306}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.10}Conclusions}{88}{section.307}}
\newlabel{sec:conclusions}{{6.10}{88}{Conclusions}{section.307}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.11}Final VERSION}{89}{section.308}}
\@writefile{toc}{\contentsline {section}{\numberline {6.12}Proposed Approach --- Open-Set Web Genre Identification}{89}{section.309}}
\newlabel{sec:approach}{{6.12}{89}{Proposed Approach --- Open-Set Web Genre Identification}{section.309}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.12.1}Nearest Neighbors Distance Ratio Classifier}{89}{subsection.310}}
\newlabel{sec:NNRD_Description}{{6.12.1}{89}{Nearest Neighbors Distance Ratio Classifier}{subsection.310}{}}
\@setckpt{Chapters/Word_embedding}{
\setcounter{page}{90}
\setcounter{equation}{11}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{3}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{6}
\setcounter{section}{12}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{5}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{float@type}{8}
\setcounter{AlgoLine}{14}
\setcounter{algocfline}{4}
\setcounter{algocfproc}{4}
\setcounter{algocf}{0}
\setcounter{definition}{5}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{0}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{48}
\setcounter{Hfootnote}{10}
\setcounter{bookmark@seq@number}{69}
\setcounter{lstlisting}{0}
\setcounter{section@level}{2}
}
